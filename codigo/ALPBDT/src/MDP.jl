#Code modified from DecisionMakingProblems.jl under the MIT licence

using Distributions:Categorical

struct State
    name::Symbol
end

struct Action
    name::Symbol
end

struct DiscreteMDP
    ğ’®   ::Vector{State}
    ğ’œ   ::Vector{Action}
    T   ::Union{Array{Float64, 3}, Nothing} # T(s, a, sâ€²)
    # R::Array{Float64, 2} # R(s, a) = âˆ‘_sâ€² R(s, a, sâ€²) * T(s, a, sâ€²)
    R   ::Function
    Î³   ::Float64
end

function transition(mdp::DiscreteMDP, s::Int, a::Int)
    return Categorical(mdp.T[s, a, :])
end

# # Lineworld expl
# ğ’œ = [Action(:forward), Action(:back)]
# ğ’® = [State(Symbol("Hex", i)) for i = 1:4]
# function R(s::State, a::Action)
#     if a.name === :forward
#         1
#     else
#         -1
#     end
# end

# hw = DiscreteMDP(ğ’®, ğ’œ, nothing, R, 0.8)

function lookahead(P::DiscreteMDP, U, s::State, a::Action)
    S, R, Î³ = P.ğ’®, P.R, P.Î³
    return R(s, a) + Î³ * sum(U[i] for (i, sâ€²) âˆˆ enumerate(S))
end

struct ValueFunctionPolicy
    P
    U
end

function greedy(P::DiscreteMDP, U, s::State)
    u, a = findmax(a -> lookahead(P, U, s, a), P.ğ’œ)
    return (a=a, u=u)
end

(Ï€::ValueFunctionPolicy)(s::State) = Ï€.P.ğ’œ[greedy(Ï€.P, Ï€.U, s).a]

function tensorform(P::DiscreteMDP)
    ğ’®, ğ’œ, R = P.ğ’®, P.ğ’œ, P.R
    ğ’®â€² = eachindex(ğ’®)
    ğ’œâ€² = eachindex(ğ’œ)
    Râ€² = [R(s, a) for s âˆˆ ğ’®, a âˆˆ ğ’œ]
    return ğ’®â€², ğ’œâ€², Râ€²
end