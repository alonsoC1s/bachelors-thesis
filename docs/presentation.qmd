---
title: Soluciones aproximadas al problema de Aprendizaje por Refuerzo via Programación Lineal;
lang: es-mx
subtitle: con aplicaciones al Aprendizaje Supervisado
author: Alonso Martínez Cisneros
institute: Instituto Tecnológico Autónomo de México
execute: 
  echo: false
  warning: false
format:
  revealjs:
    output-file: index.html
    theme: nord_theme.scss
    logo: img/logo_ITAM.png
    incremental: true
    revealjs-plugins:
      - presentextras/fullscreen
ascii: true
navigation-mode: grid
bibliography: BachelorsThesis.bib
---

:::{.hidden}
```{julia}
using Plots
using Random

Random.seed!(1)
include("../codigo/nord.jl")
theme(:ggnord)
plotly()
```
$$
\newcommand{\R}{\mathbb{R}}
$$
:::

# Contexto

- Aprendizaje por Refuerzo y Aprendizaje Supervisado

:::{.notes}
- La I.A está muy de moda
- Múltiples disciplinas
:::

# Les presento: Aprendizaje de Máquina

## Intro express a árboles de clasificación

- Aprendizaje Supervisado aprovecha técnicas estadísticas para predecir
- Hoy nos vamos a enfocar en la tarea de clasificación

. . .

| Edad | Colesterol | Sexo | ... | Infarto |
|-----:|:-----------|------|-----|---------|
| 65   |  150 mg/dL | M    |     | No      |
| 56   |  350 mg/dL | M    |     | Si      |
| 32   |  200 mg/dL | F    |     | No      |
| ...  |  ...       | ...  |     | ...     |

---------

### Árboles de clasificación

- Los árboles de clasificación son una de las técnicas más socorridas
- Clasifican haciendo preguntas
- Cómo sabemos que preguntas hacer, y en qué orden?

:::{.notes}
- Las preguntas son del estilo...
- Las preguntas empiezan a agrupar individuos
:::

-------

- El árbol gana utilidad después de ser "ajustado"
- En el ajuste determinamos qué variables importan y dónde hacer puntos de corte
- Vamos a pensar más en abstracto

. . .

```{julia}
X = rand(300, 2)
y = (X[:, 1] .> 0.25) .& (X[:, 1] .< 0.75) .& (X[:, 2] .> 0.25) .& (X[:, 2] .< 0.75)

# Randomly flip some labels
mask = shuffle(1:size(X, 1))[end-8:end]
y[mask] .= .~y[mask]

shapes = fill(:circle, size(y))
shapes[y] .= :diamond

push!(y, true)

scatter(X[:, 1], X[:, 2],
    zcolor = Float64.(y),
    markershape = shapes,
    legend = :none,
    xlabel = "Variable 1",
    ylabel = "Variable 2"
)
```

:::{.notes}
- Decimos "ajuste" porque conceptualmente es literal eso
- Las preguntas dividen a los individuos en 2 grupos
:::

----------

- La técnica tradicional de ajuste es un proceso en pasos

. . .

:::{.callout-tip}
No podemos ver el futuro!
:::

- Compromiso: Tomamos la mejor decisión en el presente, esperemos lo sea a futuro

:::{.notes}
- Saber el mejor corte es un proceso recursivo
:::

--------

![](img/ctree.svg){fig-align="center"}

:::{.notes}
- Este es el arbol resultante
- Explicar estructura
:::

## Aprendizaje por Refuerzo

- No tenemos el lujo de tener millones de ejemplos
- Aprendemos interactuando con el ambiente (como los bebés)

-----------

![](img/board.svg){fig-align="center"}

-----------

![](img/diagram-start.svg){fig-align="center"}

:::{.notes}
- Conceptualizamos pensando en las posibles transiciones
- La máquina no conoce la mecánica, ni puede hacerlo
:::

----------

![](img/transicion-markov.svg){fig-align="center"}

:::{.notes}
- La máquina debe decidir que hacer con base en una _política_
- La decisión de comprar o no, determina nuestro espacio de acción a futuro
- Los humanos podemos usar este diagrama para ver la casilla más visitada, la
  máquina no puede
:::

-----------

- La máquina debe _ajustar_ su forma de jugar partida a partida

. . .

:::{.callout-tip}
Cuando hablamos de aprender, hablamos de esta habilidad de ajustar con base en
las cosas que va aprendiendo partida a partida, y los resultados que eso tuvo en
su objetivo.
:::

- No hace falta conocer las reglas del juego ni entender la dinámica

## Haciendo conexiones

- Que magia permite a la máquina saber qué cambiar?
- Podemos hacer el análisis a mano

. . .


![](img/hist-miniopoly.svg){fig-align="center"}

----------

- Qué estamos tratando de representar en la gráfica?
- Funcion estado-valor
- Para ejemplos más complejos este análisis es imposible

:::{.notes}
- La mágia está en encontrar proceso iterativos de mejora
:::

----------

### Los personajes principales

- Estados
- Políticas
- Funciones estado-valor

. . .

Nuestros protagonistas son la función estado-valor y acción valor. 

$$
\begin{align*}
v_{\pi} (s) &:= \mathbb{E}_{\pi} \left[\sum_{k=0}^{\infty} \gamma^{k}
R_{t+k+1} \, \middle\vert \, S_{t} = s\right], \\
q_\pi (s, a) &:= \mathbb{E}_{\pi} \left[ \sum_{k=0}^{\infty} \gamma^{k}
R_{t+k+1} \,\middle\vert\, S_t = s, A_t = a \right].
\end{align*}
$$

:::{.notes}
- Por qué nos interesan?
:::

----------

- Nuestro objetivo es encontrar los valores más grandes posibles de $v$
- A esa función la llamamos $v_{*}(s)$

. . .

$$
v_{*}(s) = \max_{a} q_\pi (s, a)
$$

- Quién garantiza que siquiera existe??

# La optimización como un problema geométrico de búsqueda

## {fullscreen=true}

```{julia}
#| fig-align: center
#| output-location: slide
Plots.surface(-10:10, -10:10, (x, y) -> x^2 - y^2, colorbar=:none)
```

:::{.notes}
- Las funciones de interés son muy diferentes a las que conocemos
- Viven en espacios difíciles de visualizar
- Empezamos con una función común, la nuestra no es asi
:::

## {fullscreen=true}

```{julia}
paraboloid(x, y) = x^2 - y^2
Plots.surface(-10:10, -10:10, paraboloid, colorbar=:none, camera=(45,30))
scatter!([(5, 0, 0), (0, 5, 0)], markersize=2)
```

:::{.notes}
- Nuestra función solo existe en estados
:::

## {fullscreen=true}

En estos puntos la función toma estos valores.

```{julia}
Plots.surface(-10:10, -10:10, paraboloid, colorbar=:none, camera=(45,30))
points_on_surf = [(p[1], p[2], paraboloid(p[1], p[2])) for p in [(5, 0), (0, 5)]]
scatter!(
    points_on_surf,
    markersize=2
)
```

## {fullscreen=true}

Vamos a representar esa función compleja como algo simple: un punto en el plano

```{julia}
scatter(
    [(points_on_surf[1][3], points_on_surf[2][3])],
    xlim = (0,50),
    ylim = (-50, 0),
    legend = :none,
)
```

:::{.notes}
- Este es el mismo principio que nos ayudará a pensar en la función estado-valor
:::

## Quién dice que lo que buscamos existe?

Las ecuaciones de optimalidad de Bellman

$$
v_\pi (s) = \sum_{a} \pi(a \mid s) \left[ r(s,a) + \gamma \sum_{s'} p(s' \mid s, a) v_\pi (s') \right].
$$

:::{.notes}
- Notar la recurrencia
- Aseguran también la unicidad
- Esto es en realidad una ecuación para cada estado
- Pensamos en la función como un vector para hablar de todas las ecuaciones a la vez
:::

-----------

El gran regalo: Operador de Bellman

$$
T_\pi \vec{v} = \vec{R}_\pi + \gamma \vec{P}_\pi \vec{v}.
$$

- Nos permite pensar en todas las ecuaciones al mismo tiempo
- Lo podemos pensar como una cosa que transforma vectores

:::{.notes}
- Tiene toda la cara de cómo definimos $v$ desde el inicio, pero para vectores
- Es un sistema lineal pero es enooorme
:::

## {fullscreen=false}

- El operador de Bellman tiene propiedades muy bonitas
    - Es una contracción!
    - Un teorema de Banach

. . .

:::{.callout-tip}
Si sabemos hacia dónde se están encogiendo los puntos, no tenemos que explorar
todo el espacio. Podemos establecer fronteras dentro de las cuales **tiene** que
estar la solución
:::

. . .

```{julia}
#| fig-align: center
using Polyhedra
h = HalfSpace([1, 0], 10) ∩
    HalfSpace([0, 1], 10) ∩
    HalfSpace([1, 1], 13) ∩
    HalfSpace([-1, 0], 0) ∩
    HalfSpace([0, -1], 0)

p = polyhedron(h)

plot(p, ratio=:equal,
    xlabel = "Estado 1",
    ylabel = "Estado 2",
    xlims = (-1, 11),
    ylims = (-1, 11),
    alpha = 0.6,
)
```

:::{.notes}
- Por qué son bonitos los polítopos
:::


--------

- Si aseguramos que la función objetivo es lineal, las soluciones están en las esquinas del polítopo!

. . .

:::{.callout-tip}
Es mucho más rápido explorar solo las esquinas que todo el interior de un políedro
:::

- Con todo y estas reducciones, la dimensión del espacio sigue siendo demasiado
  grande para explorar

## Usando una aproximación lineal

- Reemplazamos la función de interés con una aproximación "de baja resolución"
- Escogemos una cantidad "pequeña" de dimensiones para explorar, no todas
- Si sabemos escoger, podemos encontrar la solución exacta
- Aún si no logramos abarcar la solución, @farias2004constraint pone límites al error

# La conexión prometida

- En qué se parecen el Aprendizaje Supervisado y por refuerzo?
- @xiong ya habían explorado usar RL, pero con redes neuronales (RLBDT)
- (RLBDT) obtiene resultados muy prometedores en calidad de ajuste y tiempo usado.

:::{.notes}
- Una de las ideas semi-originales de este trabajo
:::

---------

Convertimos este algoritmo

![](img/alg_7-1.png)

en algo que un máquina haría sola

# Conclusiones y trabajo a futuro

- Propuse Approximate Linear Programming Based Decission Trees (ALPBDT) como
  técnica "nueva"
- No hay implementación completa (aún)
- Técnicas como XGBoost @XGBoost dominan el modelado predictivo, vale la pena
  investigar en árboles.

# References

::: {#refs}
:::