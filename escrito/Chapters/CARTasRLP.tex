\chapter{\textsc{cart} fitting as a Reinforcement Learning Problem}
\label{chapter:CARTasRLP}

In chapter \ref{chapter:SupervisedLearning} we reviewed the basic process of
fitting a classification tree: what it entails, the algorithms, and some
computational complexity calculations. To some readers it might be obvious how
the process described by algorithms \ref{alg:tree-fit} and, most importantly
\ref{alg:best-greedy-binary-split}, are similar to the exemplary RL problems we
have developed thus far. In this chapter we fill in the gaps and formalize how
the CART fit procedure can be cast as a MDP and thus how the techniques
discussed in part \ref{part:II} may be used and what we could expect from them.

Let us begin by stating something that might not have been obvious when
describing the fitting procedure in chapter \ref{chapter:SupervisedLearning}:
the algorithms described and used in practice rarely---if ever---produce the
best possible tree. The algorithms are, as their names suggest, greedy. As a
reminder, when using the term greed we mean that during some optimization
procedure such as solving an RL problem or fitting a CART we choose the best
action available to us at the moment instead of exploring all the possible
rammifications and then selecting the one that is most convenient at the end of
the line. The alternative to greedy algorithms are lookahead algorithms, which
work by ``looking ahead'' into the future and evaluating the effects a certain
action in the present might have some steps in the future. The \emph{true} best
fit of a tree as opposed to a greedy best would require the lookahead to be
carried out for each split made until all subsplits reach a termination
condition. In the following section we develop the lookahead procedure in
detail, and set the basis for this chapter's main ideas.

\section{True best splitting procedures}

Algorithm \ref{alg:tree-fit} was described as the procedure for greedily fitting
a binary classification tree, but strictly speaking that formulation can just as
well describe a lookeahead fitting procedure. The splitting procedure described
in algorithm \ref{alg:best-greedy-binary-split} for finding the best binary
split is the actual greedy procedure. Let us describe formally what a
non-greedy, lookahead-based splitting procedure might look like. To avoid
confusion from now on we will be using explicitly the terms \emph{greedy} to
refer to the already described fitting procedure and its resulting splits, and
\emph{true best} to refer to the splits obtained via lookahead. 

Algorithm \ref{alg:true-best-binary-split} aims to describe the procedure to
find the true best split for a $\L_t$ a subset of the training set. Recalling
some notation, $\mathcal{X}_t$ refers to the input space corresponding to the
partition $\L_t$ of the training set, $X_j$ refers to the $j$-th feature
variable(in other words, the $j$-th entry on the sample vectors). To ease the
description of the next algorithm we introduce some new notation. We denote
$\mathcal{X}_{t}^{j} \coloneqq \left\{ x_{j, o} \mid \vec{x}_{o} \in
\mathcal{X}_t \right\}$ the set of values of the feature $X_J$ contained in
$\mathcal{X}_t$. In other words, the set of observed values for the feature
$X_j$ contained in $\L_t$ the subset of the training set. For a refresher on
terminology and notation please refer to chapter
\ref{chapter:SupervisedLearning} section \ref{sss:formalizing-trees}.

\begin{algorithm}
    \SetKwFunction{TrueBestSplit}{FindTrueBestBinarySplit}
    \KwIn{A subset $\L_t$ of the training set $\L$}
    \KwOut{The true best binary split $s_*$ on $\L_t$ and impurity decrease achieved $\Delta_{*}$.}
    \Function{\TrueBestSplit{$\L_t$}}{
        $\Delta_* \gets - \infty$ \;
        \For{$j \gets 1, \dots, p$}{
            \ForEach{observed value $x_{j, i} \in \mathcal{X}_{j}^{t}$}{
                Set $s \gets x_{j, i}$ the split value that defines node $t$ \;
                \If{stopping criteria are met}{
                    \Return{$(\Delta i(s, t), s)$}
                }
                Partition $\L_t$ into $\L_{t_L}, \L_{t_R}$ according to $s$ \; 
                $\Delta_{L}^{*} \gets$ \TrueBestSplit{$\L_{t_L}$} \;
                $\Delta_{R}^{*} \gets$ \TrueBestSplit{$\L_{t_R}$} \;
                \If{$\Delta i(s, t) + \Delta_{L}^{*} + \Delta_{R}^{*} > \Delta$ \label{value-calculation}}{
                    $\Delta \gets \Delta i(s, t) + \Delta_{L}^{*} + \Delta_{R}^{*}$ \;
                    $s_{*} \gets s = x_{j, i}$
                }
            }
        }
        \Return{$(\Delta_*, s_*)$} \;
    }
    \caption[True best binary split for node $t$.]{True best binary split $s_*$ for node $t$.}
    \label{alg:true-best-binary-split}
\end{algorithm}

Notice how the function \TrueBestSplit calls itself twice with the subsets
$\L_{t_R}, \L_{t_L}$, created by the split $s$, as arguments. In simple words
algorithm \ref{alg:true-best-binary-split} tries each observed value for each
feature in it's own subset of the training set $\L_t$ as a possible splitting
point. Then, to decide if the split is good, calculates the best maximum
impurity decrease for the subsets defined by the left and right children of the
node created by the current split. To calculate that maximum impurity decrease
we once again explore every split possible for the given, strictly smaller,
subset of $\L_t$. Since the explored subsets get progressively smaller in terms
of observations contained, it is intuitively clear that the splitting will stop
eventually, given that we check for stopping criteria satisfaction. One example
of a stopping criteria is, stop splitting if the current $\L_t$ has less
than $\kappa$ samples.

Even though the recursive splittin procedure does eventually stop thanks to the
stopping criteria, the call stack for \TrueBestSplit becomes very deep very
quickly. Once the call stack reaches a maximum dept, it starts to grow in
breadth since we only just begun testing a single split $s$, when we must test
for every possible one. As mentioned in chapter
\ref{chapter:SupervisedLearning}, in the case of binary splits we would explore
$2^{N -1}-1$, possible partitions. Each corresponding to a call to
\TrueBestSplit. This amount of complexity makes all but trivial problems
impossible to solve. 

\subsection{Making connections}

The original tree fitting algorithm \ref{alg:tree-fit} described in
\ref{chapter:SupervisedLearning} calls to find the split $s_*$ on $\L_t$ that
maximizes impurity decrease
\[
    s_* = \argmax_{s \in \mathcal{Q}} \Delta i(s, t).
\]
In the same chapter we reviewed how we \emph{approximate} the solution to that
maximization problem by adopting a greedy strategy captured in the function
\GreedyBestSplit. Then, in the algorithm \ref{alg:true-best-binary-split} we
described how to find the exact solution to that same optimization problem. In
the line \ref{value-calculation} of algorithm \ref{alg:true-best-binary-split}
we are comparing the decrease in impurity the currently explored split would
achieve.  For that to be calculated exactly, we must calculate recursively the
impurity decrease we would hope to achieve for each partition.

This dependence of the current value of impurity decrease on the impurity
decrease on the subsets generated by the current split feels very reminiscent of
the defining characteristics of value functions in a RL problem. The
calculations in line \ref{value-calculation} of the function \TrueBestSplit can
be formulated as
\[
    \Delta i(s, t) + \sum_{s_L} \Delta i(s_L, t) + \sum_{s_R} \Delta i(s_R, t),
\]
where $s_L$ and $s_R$ are all possible splits on the left and right subsets of
$\L_t$ respectively. With some attention it becomes clear that the previous
equation is a special case of the recurrence relationship described by
\eqref{eq:bellmans-recurrence-prime}, which is
\[
    v_\pi (s) = 
    \sum_{a} \pi(a \mid s) \left[ 
        \underbrace{r(s,a)}_{\Delta i(s, t)} 
        + \gamma \sum_{s'} p(s' \mid s, a) \underbrace{v_\pi (s')}_{\Delta i(s', t)} 
    \right].
\]
Removing the sum over $a$, and considering both a deterministic policy ($\pi(a
\mid s) = 1$) and deterministic transitions, we see that both equations
represent exactly the same idea: recursive evaluation. By calculating impurity
decrease the way we did on \TrueBestSplit, we were effectively leveraging
Bellman's recurrence relationship with impurity decrease $\Delta i(s, t)$ as our
value function without realizing it.

Now that we have shown how the CART fitting procedure and an RLP are analogous
and motivated why the structure of the value function in this context warants
the use of RL techniques, let us proceed to define carefully the state and
action spaces for this particular problem.

\section{Sketching implementations}

To implement ALP to solve the CART fitting procedure we must explicitly define
some of the things we need to know to solve a RL problem.

The set of states $\States$ is the set of all possible partitions of the
training set $\L$. In other words, the set of nodes $t$ defined by the possible
splits $s \in \mathcal{Q}$. The set of actions $\Actions$ is the set of all
possible splits on our current subset of $\L$. In the notation introduced
previously, $\mathcal{Q}$ the set of all questions that define binary splits.
The set of questions can be described as $\mathcal{Q} = \{ \text{is } x \leq
x_{j, i} \text{ ?} \mid x_{j, i} \in \mathcal{X}_{j}^{t}, \; \forall j = 1,
\dots, p \}$.  The innermost loop in algorithm \ref{alg:true-best-binary-split}
is ennumerating all the actions in the set of available actions. Since choosing
a split $s$ on a given subset $\L_t$ always results in the same partion into
$\L_{t_L}$ and $\L_{t_R}$ being made, the transition function $p(s' \mid s, a)$
is not necessary for this problem. Finally, the expected reward function for a
state $s$ and action $a$, $r(s, a)$, is precisely the impurity decrease function
$\Delta i(s, t)$ for a split $s$ and node $t$. The value function is also the
impurity decrease function $\Delta i (s, t)$.

The Exact Linear Program for this problem is
\begin{equation}
\begin{array}{rl@{}ll}
    \displaystyle \min & \displaystyle \sum_{s \in \mathcal{Q}} \Delta i(s, t) \\
    \text{Subject to} & \displaystyle \Delta i(s, t) \geq \Delta i(s, t') + \sum_{s'} \Delta i(s', t') & \quad \forall t' \in \Actions, s \in \States. \\
\end{array}
\end{equation}

Ahora nom√°s falta definir las basis functions $\Phi$ y describir el ALP.

For now we refrain from selecting a basis $\Pi$ of functions. The choice of this
basis is highly heuristic and relies on experience with the specific problem
being solved as pointed out by \citeauthor{farias2003LP2ADP} in
\cite{farias2003LP2ADP}. Likewise, the specific parameters reviewed in chapter
\ref{chapter:PropertiesGuarantees} for the Reduced Linear Program are selected
on a case-by-case basis \cite{farias2004constraint}. Both the basis functions
and the parameters to carry out constraint sampling can be considered
hyperparameters like tree depth and minimum nodes per leaf in the example of
tree fitting. As such, these may be optimized by cross validation to obtain the
best possible results to the problem at hand.  

\section{Closing thoughts}

The idea of applying Reinforcement Learning to the tree fitting procedure is not
particularly novel. The approach has been explored for instance by
\citeauthor{xiong} in \cite{xiong}. In their article the authors try to learn
optimal splitting strategies with a Recurrent Neural Network controller trained
by RL to maximize impurity decrease with the Gini impurity function. They
baptize their algorithm as RLBDT (Reinforcement Learning Based Decission Trees).
The article concludes that RLBDT outperforms the traditional greedy fitting
procedure described in CART (algorithm \ref{alg:tree-fit} using the splitting
strategy in algorithm \ref{alg:best-greedy-binary-split}).

Previous work laid out by \citeauthor{xiong} suggests there is some merit to the
idea of using RL for the problem described in this chapter. However, the
approach taken, which is utilizing Recurrent Neural Networks, is very different
from the approach we took. This chapter was developed almost entirely without
external references, leveraging only the concepts reviewed earlier. In the next
and final chapter of this thesis we discuss an implementation of CART and the
Approximate Linear Programming Based Decission Trees (ALPBDT) in Julia, and run
some test to compare the implementations.