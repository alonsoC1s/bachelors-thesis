\chapter{\textsc{cart} fitting as a Reinforcement Learning Problem}
\label{chapter:CARTasRLP}

In chapter \ref{chapter:SupervisedLearning} we reviewed the basic process of
fitting a classification tree: what it entails, the algorithms, and some
computational complexity calculations. To some readers it might be obvious how
the process described by algorithms \ref{alg:tree-fit} and, most importantly
\ref{alg:best-greedy-binary-split}, are similar to the exemplary \ac{rl}
problems we have developed so far. In this chapter we fill in the gaps and
formalize how the \ac{cart} fit procedure can be cast as an \ac{mdp} and thus how the
techniques discussed in part \ref{part:II} may be used and what we could expect
from them.

Let us begin by stating something that might not have been obvious when
describing the fitting procedure in chapter \ref{chapter:SupervisedLearning}:
the algorithms described and used in practice rarely--if ever--produce the
best possible tree. The algorithms are, as their names suggest, greedy. As a
reminder, when using the term greed we mean that during some optimization
procedure such as solving an \ac{rl} problem or fitting a \ac{cart} we choose
the best action available to us at the moment instead of exploring all the
possible rammifications and then selecting the one that is most convenient at
the end of the time. The alternative to greedy algorithms are lookahead
algorithms, which work by ``looking ahead'' into the future and evaluating the
effects a certain action in the present might have some steps in the future. The
\emph{true} best fit of a tree as opposed to a greedy best would require the
lookahead to be carried out for each split made until all subsplits reach a
termination condition. In the following section we develop the lookahead
procedure in detail, and set the basis for this chapter's main ideas.

\section{True best splitting procedures}

Algorithm \ref{alg:tree-fit} was described as the procedure for greedily fitting
a binary classification tree, but strictly speaking that formulation can just as
well describe a lookeahead fitting procedure. The splitting procedure described
in algorithm \ref{alg:best-greedy-binary-split} for finding the best binary
split is the actual greedy procedure. Let us describe formally what a
non-greedy, lookahead-based splitting procedure might look like. To avoid
confusion from now on we will be using explicitly the terms \emph{greedy} to
refer to the already described fitting procedure and its resulting splits, and
\emph{true best} to refer to the splits obtained via complete lookahead. 

Algorithm \ref{alg:true-best-binary-split} aims to describe the procedure to
find the true best split for some $\L_t$, a subset of the training set.
Recalling some notation, $\mathcal{X}_t$ refers to the input space corresponding
to the partition $\L_t$ of the training set, $X_j$ refers to the $j$-th feature
variable (in other words, the $j$-th entry of the sample vectors $\vec{x} \in
\R^{p}$). To ease the description of the next algorithm we introduce some new
notation. We denote $\mathcal{X}_{t}^{j} \coloneqq \left\{ x_{j, o} \mid
\vec{x}_{o} \in \mathcal{X}_t \right\}$ the set of values of the feature $X_j$
contained in $\mathcal{X}_t$. 

Recalling the setting of chapter \ref{chapter:ReinforcementLearning}, the
elements of the training set are tuples $(\vec{x}_i, y_i)$. The vectors
$\vec{x}_i \in \R^{p}$ can be conceptualized as lists of measured values for $p$
variables of interest carried out for the $i$-th individual. For the remaining
notation used in the algorithm below, please refer to chapter
\ref{chapter:SupervisedLearning} section \ref{sss:formalizing-trees}.

\begin{algorithm}
    \SetKwFunction{TrueBestSplit}{FindTrueBestBinarySplit}
    \KwIn{A subset $\L_t$ of the training set $\L$}
    \KwOut{The true best binary split $s_*$ on $\L_t$ and impurity decrease achieved $\Delta_{*}$.}
    \Function{\TrueBestSplit{$\L_t$}}{
        $\Delta_* \gets - \infty$ \;
        \ForEach{feature $X_j$ with $j \in  \{1, \dots, p\}$}{
            \ForEach(\tcc*[f]{Iterate over the observed values of feature the $X_j$ contained in $\L_t$.}){$x_{j, l} \in \mathcal{X}_{t}^{j}$}{
                Set $s \gets x_{j, l}$ the split value that separates node $t$ \;
                \If{stopping criteria are met}{
                    \Return{$(\Delta i(s, t), s)$}
                }
                Partition $\L_t$ into $\L_{t_L}, \L_{t_R}$ according to $s$ \; 
                $\Delta_{L}^{*} \gets$ \TrueBestSplit{$\L_{t_L}$} \;
                $\Delta_{R}^{*} \gets$ \TrueBestSplit{$\L_{t_R}$} \;
                \If{$\Delta i(s, t) + \Delta_{L}^{*} + \Delta_{R}^{*} > \Delta$ \label{value-calculation}}{
                    $\Delta_* \gets \Delta i(s, t) + \Delta_{L}^{*} + \Delta_{R}^{*}$ \;
                    $s_{*} \gets s = x_{j, i}$
                }
            }
        }
        \Return{$(\Delta_*, s_*)$} \;
    }
    \caption[True best binary split for node $t$.]{True best binary split $s_*$ for node $t$.}
    \label{alg:true-best-binary-split}
\end{algorithm}

Notice how the function \TrueBestSplit calls itself twice with the subsets
$\L_{t_R}$ and $\L_{t_L}$, created by the split $s$, as arguments. In simple words
algorithm \ref{alg:true-best-binary-split} tries each observed value for each
feature in its own subset of the training set $\L_t$ as a possible splitting
point. Then, to decide if the split is good, calculates the best maximum
impurity decrease for the subsets defined by the left and right children of the
node created by the current split. To calculate that maximum impurity decrease
we once again explore every split possible for the given, strictly smaller,
subset of $\L_t$. Since the explored subsets get progressively smaller in terms
of observations contained, it is intuitively clear that the splitting will stop
eventually, given that we check for stopping criteria satisfaction. One example
of a stopping criterion is, stop splitting if the current $\L_t$ has less
than $\kappa$ samples.

Even though the recursive splitting procedure does eventually stop thanks to the
stopping criteria, the call stack for \TrueBestSplit grows high enough to
overflow for all practical problems. Once the call stack reaches a maximum
depth, provided it does not exceed the computer's maximum stack size, it starts
to grow in breadth since we only just begun testing a single split $s$, when we
must test for every possible one. If the problem is complex and the call stack
grows higher than the maximum call stack size for the computer, the program will
crash and return nothing useful. As mentioned in chapter
\ref{chapter:SupervisedLearning}, in the case of binary splits we would explore
$2^{|\L| -1}-1$, possible partitions.  Each corresponding to a call to
\TrueBestSplit. This amount of complexity makes all but trivial problems
impossible to solve. 

\subsection{Making connections}

The original tree fitting algorithm \ref{alg:tree-fit} described in chapter
\ref{chapter:SupervisedLearning} calls to find the split $s_*$ on $\L_t$ that
maximizes the impurity decrease
\begin{equation}
    \label{eq:greedy-split-condition}
    s_* = \argmax_{s \in \mathcal{Q}} \Delta i(s, t).
\end{equation}
In earlier chapters we too obtained optimal policies by taking the action that
maxized some criteria of optimality. Specifically, in chapter
\ref{chapter:ApproximateLinearP}, to prove Theorem
\ref{thrm:opt-policy-existence} we constructed optimal policies using equation
\eqref{eq:greedy-policy-construction}. Concretely, we created an optimal policy
$\pi_*$ by defining it in an action-to-action basis as follows:
\begin{align*}
    \pi_{*} (s) &= \argmax_{a} q_{*} (s, a) \quad \forall s \in \States \\
    &= \argmax_a r(s, a) + \sum_{s'} v_* (s'),
\end{align*}
where the last identity is a consequence of \eqref{eq:q-by-v}. In simple words,
we establish that the policy $\pi_*$ will assign to state $s$ the action $a_*$
that maximizes $q_* (s, a)$. 

From now on we use the symbol $\bar{s}$ to denote splits to avoid confusing it
with the symbol $s$ which is used to denote states. Looking closely at
\eqref{eq:greedy-split-condition} we note that the idea for creating an optimal
policy (in this case, the splits defining a tree) is very similar to the
strategy used to prove Theorem \ref{thrm:opt-policy-existence}. The impurity
decrease function calculates the reward to be obtained by splitting node $t$
using the split $\bar{s}$. In other words, it is a function of both the state (a
node $t$) and some action (splitting using $\bar{s}$). This makes it analogous
to the \emph{expected reward function} defined in Definition
\ref{dfn:rewards-func}.

By recognizing that:
\begin{itemize}
    \item The nodes $t$ we reviewed in the tree fitting algorithm correspond to
        the states $s \in \States$ of a \ac{rl} problem,
    \item The splits $\bar{s} \in \mathcal{Q}$ correspond to the actions $a \in
        \Actions$ of a \ac{rl} problem,
    \item The impurity decrease function $\Delta i(\bar{s}, t)$ is analogous to
        the expected rewards function,
\end{itemize}
it becomes clear that the tree fitting procedure is indeed a special case of the
general \ac{rl} problem we defined in chapter
\ref{chapter:ReinforcementLearning}. As a consequence, we can now recognize why
algorithm \ref{alg:tree-fit} cannot obtain true best trees: it focuses on
maximizing immediate reward rather than analyzing long-term payoff. That is why
algorithm \ref{alg:true-best-binary-split} analyzes the rammifications of every
possible split. In order to find true-best splits, it is not enough to focus
only on the immediate actions available.

There is just one piece missing from the formulation of a \ac{rl} problem: the
state-value function $v(s)$. This piece is crucial, since approximating it is
the cornerstone of Approximate Linear Programming. In the case of tree fitting,
the value function is the impurity function $i(t)$ of a node $t$. This makes
intuitive sense, given that impurity is the measure we use to determine if a
split was good or bad, and it is a function over the set of states, which is the
space of possible nodes in this case. With this in mind, we can define the
action-value function for the tree fitting problem:
\begin{equation}
    q (t, \bar{s}) = \Delta i(\bar{s}, t) + \sum_{t'} i (t').
\end{equation}

So far we have avoided explicitly describing an impurity function. The selection
of a suitable impurity function comes with some caveats that are outside the
scope of this thesis. Suffice it to say that from now on we will be using the
Gini impurity index, which we define below.

\begin{dfn}{Gini Impurity Index \cite[Ch.~8.1.2]{intro2statslearning} }{gini-index}
    The \emph{gini impurity index} or simply the \emph{gini index} is defined by
    \begin{equation}
        i_{G}(t) = \sum_{k=1}^{K} \IP{c_k \mid t} \left( 1 - \IP{c_k \mid t} \right).
    \end{equation}
    The expression $\IP{c_k \mid t}$ is the probability that some observation in
    $t$ belongs to category $c_k$.
\end{dfn}

We refer to the Gini index as an impurity index since it is a measure of how
varied the observations in a given node are. A small value indicates that a node
contains predominantly observations from a single class. Given that we want to
make nodes purer so we can make better predictions, we would like the values of
the gini index to be smaller rather than bigger. Notably, in the traditional
treatment of the \ac{rl} problem given in chapters
\ref{chapter:ReinforcementLearning} and \ref{chapter:ApproximateLinearP}, the
bigger the value function the better. Fortunately, the ALP approach developed in
chapter \ref{chapter:ApproximateLinearP} can be used without modifications.

% In the same chapter we reviewed how we \emph{approximate} the solution to that
% maximization problem by adopting a greedy strategy captured in the function
% \GreedyBestSplit. Then, in the algorithm \ref{alg:true-best-binary-split} we
% described how to find the exact solution to that same optimization problem. In
% the line \ref{value-calculation} of algorithm \ref{alg:true-best-binary-split}
% we are comparing the decrease in impurity the currently explored split would
% achieve.  For that to be calculated exactly, we must calculate recursively the
% impurity decrease we would hope to achieve for each partition.

% This dependence of the current value of impurity decrease on the impurity
% decrease on the subsets generated by the current split feels very reminiscent of
% the defining characteristics of value functions in a RL problem. The
% calculations in line \ref{value-calculation} of the function \TrueBestSplit can
% be formulated as
% \[
%     \Delta i(s, t) + \sum_{s_L} \Delta i(s_L, t) + \sum_{s_R} \Delta i(s_R, t),
% \]
% where $s_L$ and $s_R$ are all possible splits on the left and right subsets of
% $\L_t$ respectively. With some attention it becomes clear that the previous
% equation is a special case of the recurrence relationship described by
% \eqref{eq:bellmans-recurrence-prime}, which is
% \[
%     v_\pi (s) = 
%     \sum_{a} \pi(a \mid s) \left[ 
%         \underbrace{r(s,a)}_{\Delta i(s, t)} 
%         + \gamma \sum_{s'} p(s' \mid s, a) \underbrace{v_\pi (s')}_{\Delta i(s', t)} 
%     \right].
% \]
% Removing the sum over $a$, and considering both a deterministic policy ($\pi(a
% \mid s) = 1$) and deterministic transitions, we see that both equations
% represent exactly the same idea: recursive evaluation. By calculating impurity
% decrease the way we did on \TrueBestSplit, we were effectively leveraging
% Bellman's recurrence relationship with impurity decrease $\Delta i(s, t)$ as our
% value function without realizing it.

Now that we have shown how the \ac{cart} fitting procedure and a Reinforcement
Learning Problem are analogous and motivated why the structure of the value
function in this context warrants the use of \ac{rl} techniques. Let us proceed
to define carefully the state and action spaces for this particular problem.

\section{Sketching implementations}

To implement ALP to solve the \ac{cart} fitting procedure we must explicitly
define some of the things we need to know to solve an \ac{rl} problem.

As discussed last section, the set of states $\States$ is the set of all
possible partitions of the training set $\L$, which we will denote by $B_{\L}$.
The set of actions $\Actions$ is the set of all possible splits on our current
subset of $\L$. In the notation introduced previously, $\mathcal{Q}$ the set of
all questions that define binary splits. The set of questions can be described
as $\mathcal{Q} = \{ \text{is } x \leq x_{j, i} \text{ ?} \mid x_{j, i} \in
\mathcal{X}_{j}^{t}, \; \forall j = 1, \dots, p \}$.  The innermost loop in
algorithm \ref{alg:true-best-binary-split} is ennumerating all the actions in
the set of available actions. Since choosing a split $\bar{s}$ on a given subset
$\L_t$ always results in the same partition into $\L_{t_L}$ and $\L_{t_R}$ being
made, the transition function $p(s' \mid s, a)$ is not necessary for this
problem. Finally, the expected reward function for a state $s$ and action $a$,
$r(s, a)$, is precisely the impurity decrease function $\Delta i(\bar{s}, t)$
for a split $\bar{s}$ and node $t$. The value function is the impurity function
$i(t)$.

Given the equivalences we just established, we proceed to reformulate the Exact
Linear program \eqref{lp:exact-lp}, presented in chapter
\ref{chapter:ApproximateLinearP}. The \eqref{lp:exact-lp} was formulated, using
the general form of the state-value and rewards functions, by transforming:
\begin{equation*}
\begin{array}{rl@{}ll}
    \displaystyle \max_{v: \States \to \R} & v(s) \\
    \text{Subject to} & \displaystyle v(s) \leq r(s,a) + \gamma \sum_{s'} p(s' \mid s, a) \, v(s') & \quad \forall a \in \Actions, s \in \States, \\
    & v(s) \text{ unconstrained,} & \quad \forall s \in \States,
\end{array}
\end{equation*}
into the following expression, which is a Linear Program
\begin{equation*}
\tag{ELP}
\begin{array}{rl@{}ll}
    \displaystyle \max_{\vec{v} \in \R^{|\States|}} & \vec{c}^{\top} \vec{v} \\
    \text{Subject to} & \vec{v} \leq \vec{R}_a + \gamma \vec{P}_a \vec{v} & \quad \forall a \in \Actions \\
    & \vec{v} \text{ unconstrained.} &
\end{array}
\end{equation*}
The previous argument is merely a reproduction of the arguments presented in
Chapter \ref{chapter:ApproximateLinearP} to enhance clarity.

The Exact Linear Program for this problem can be formulated using the impurity
decrease and impurity by transforming the following optimization problem into a
Linear Program.
\begin{equation}
\label{eq:pre-cart-lp}
\begin{array}{rl@{}ll}
    \displaystyle \max_{i_G: B_\L \to \R} & \displaystyle - \sum_{t \in B_\L} i_{G}(t) \\
    \text{Subject to} & \displaystyle i_{G}(\bar{s}) \geq \Delta i_{G}(\bar{s}, t') + \sum_{t'} i_{G}(t') & \quad \forall t' \in B_\L, \; \bar{s} \in \mathcal{Q}. \\
    & i_{G}(t) \text{ unconstrained,} & \quad \forall t \in B_\L.
\end{array}
\end{equation}

We can transform the optimization problem in \eqref{eq:pre-cart-lp} expressing
it in terms of vectors and matrices as we did in chapter
\ref{chapter:ApproximateLinearP}. We consider the impurity function as a vector
$\vec{i}$,
\[
    \vec{i} = \left[ i_{G}(t_1), \dots, i_{G}(t_{|B_L|}) \right]^{\top}.  
\]
We also introduce $\vec{Di}_{\bar{s}} \in \R^{|B_\L|}$ defined as $[\Delta
i_{G}(t_1, \bar{s}), \dots, \Delta i_{G}(t_{|B_\L|}, \bar{s})]^{\top}$. We
originally defined the impurity decrease function $\Delta i$ in terms of a
general impurity function; now we use the notation of $\Delta i_{G}$ to make
clear that we refer to impurity decrease with respect to the Gini impurity
index.

By using the notation just introduced, we can write \eqref{eq:pre-cart-lp} as:
\begin{equation}
\tag{CART-ELP}
\label{lp:cart-elp}
\begin{array}{rl@{}ll}
    \displaystyle \max_{\vec{i} \in \R^{|B_\L|}} & - \vec{c}^{\top} \vec{i} \\
    \text{Subject to} & \displaystyle \vec{i} \geq \vec{Di}_{\bar{s}} + \vec{1} (\vec{1}^{\top} \vec{i}) & \quad \forall \bar{s} \in \mathcal{Q}, \\
    & \vec{i} \text{ unconstrained.} &
\end{array}
\end{equation}
Where $\vec{1}$ is the vector where each entry equals 1.This formulation is
nothing more than the original Exact Linear Program expressed using the
equivalences between state-value and rewards functions established in this
chapter.

For now we refrain from selecting a basis $\Phi$ of functions needed to define
the Reduced Linear Program presented as \ref{lp:reduced-lp} introduced in
chapter~\ref{chapter:PropertiesGuarantees}. The choice of this basis is
heuristic and relies on experience with the specific problem to be solved as
pointed out by \citeauthor{farias2003LP2ADP} in \cite{farias2003LP2ADP}.
Likewise, the specific parameters reviewed in
chapter~\ref{chapter:PropertiesGuarantees} for the Reduced Linear Program, such
as sampling distribuition, are selected on a case-by-case basis
\cite{farias2004constraint}. Both the basis functions and the parameters to
carry out constraint sampling can be considered hyperparameters like tree depth
and minimum nodes per leaf in the example of tree fitting. As such, these may be
optimized by cross validation \cite[Ch.~2.1]{intro2statslearning} to obtain the
best possible results for the problem at hand.  

\section{Closing thoughts}

The idea of applying Reinforcement Learning to the tree fitting procedure is not
particularly novel. The approach has been explored, for instance, by
\citeauthor{xiong} in \cite{xiong}. In their article the authors try to learn
optimal splitting strategies with a Recurrent Neural Network controller trained
by \ac{rl} to maximize impurity decrease with the Gini impurity function. They
name their algorithm \ac{rlbdt}. The article concludes that \ac{rlbdt}
outperforms the traditional greedy fitting algorithm known as \ac{cart}.

Previous work laid out by \citeauthor{xiong} suggests there is some merit to the
idea of using \ac{rl} for the problem described in this chapter. However, the
approach taken, which is utilizing Recurrent Neural Networks, is very different
from the approach we took. This chapter was developed almost entirely without
external references, leveraging only the concepts reviewed earlier. In the next
and final chapter of this thesis we lay the foundations that would facilitate a
future, complete, implementation of \ac{alpbdt}. We will be using the
programming language Julia for several reasons. First, the syntax of the
language is particularly well suited to express mathematical ideas and will
allow us to use a similar notation to the one used so far. Additionally, Julia
enjoys performance comparable to compiled languages and has access to some of
the industry-standard mathematical optimization tools, which allowed us to
perform small-scale tests.

As mentioned in the introductory chapters problems in the area of machine
learning nowadays are mostly a problem of engineering, and there is a good
reason for that switch. Even having developed the theory behind a possibly new
algorithm, there are many practical concerns we would need to explore to provide
a complete, ready-to-use implementation of \ac{alpbdt}. These details are what
will determine an algorithm's usefulness in practice, as they can impact
accuracy, computational complexity, and other factors that are important in
industrial applications. That being said, we have developed enough theoretical
background to illustrate the concept in code.
