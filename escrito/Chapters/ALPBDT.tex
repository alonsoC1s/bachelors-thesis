\chapter{Approximate Linear Programming Based Decission Trees}
\label{chapter:ALPBDT}

In chapter \ref{chapter:CARTasRLP} we showed how the \ac{cart} fitting procedure
can be cast an \ac{mdp} and consequently tackled as a Reinforcement Learning
Problem. We concluded that the state space $\mathcal{S}$ corresponds to the set
of all partitions of the training set $\L_t$; the action space coresponds to the
set of all observed values for each available feature, and the state-value
function is the Gini impurity index. In this chapter we lay the foundations
necessary for a future implementation of \acf{alpbdt} in Julia.  We use Julia
because it allows for an efficient implementation, ease of use, and enjoys some
of the best ecosystems for optimization problems.

To represent \iac{mdp}, we define some types. Using the code in
\cite{kochenderfer2022} as inspiration, listing \ref{lst:structs} presents the
code needed to represent \iac{mdp}. The state and action spaces are represented
using the \lstinline{Vector} data structure, which in Julia are arrays allocated
contiguously. The probability transition function is represented as a
3-dimensional \lstinline{Matrix}, since it aims to capture $p(s \mid s', a)$, a
function of three arguments. This is represented as a \lstinline{Union} of a 3-d
\lstinline{Matrix} and \lstinline{Nothing} to allow for problems that have
determinisitc transitions.

\begin{lstlisting}[
    language=Julia, caption={Type definition of an \ac{mdp}}, label={lst:structs}
    ]
struct State name::Symbol end

struct Action name::Symbol end

struct DiscreteMDP
    ð’®   ::Vector{State}
    ð’œ   ::Vector{Action}
    p   ::Union{Array{Float64, 3}, Nothing}
    R   ::Function
    Î³   ::Float64
end
\end{lstlisting}

In listing \ref{lst:basic-funcs} we implement a lookahead function that will be
needed later to solve the RL problem. The listing also contains the definition
of a policy function.

\begin{lstlisting}[
    language=Julia, caption={Basic \ac{mdp} functionality},
    label={lst:basic-funcs}
    ]
function lookahead(P::DiscreteMDP, U, s::State, a::Action)
    S, R, Î³ = P.ð’®, P.R, P.Î³
    return R(s, a) + Î³ * sum(U[i] for (i, sâ€²) âˆˆ enumerate(S))
end

struct ValueFunctionPolicy
    P   ::DiscreteMDP
    U   ::Function
end

function greedy(P::DiscreteMDP, U, s::State)
    u, a = findmax(a -> lookahead(P, U, s, a), P.ð’œ)
    return (a=a, u=u)
end

(Ï€::ValueFunctionPolicy)(s::State) = Ï€.P.ð’œ[greedy(Ï€.P, Ï€.U, s).a]
\end{lstlisting}

Listing \ref{lst:LP-form} deals with the Exact Linear Programming formulation
discussed in chapter \ref{chapter:ApproximateLinearP}.

Listing \ref{lst:LP-form} presents how the vectors and matrices needed to define
the Reduced Linear Program associated with the \ac{cart} fitting procedure
\eqref{lp:cart-elp} can be computed. These will be necessary later on to solve
the Linear Program. Julia's built-in function \lstinline{eachindex} returns an
iterable of efficient indexes that can be used to iterate over the structure
passed as its argument.

\begin{lstlisting}[
    language=Julia,
    caption={Linear Programming formulation for an RL Problem},
    label={lst:LP-form}
]
function tensorform(P::DiscreteMDP)
    ð’®, ð’œ, R = P.ð’®, P.ð’œ, P.R
    ð’®â€² = eachindex(ð’®)
    ð’œâ€² = eachindex(ð’œ)
    Râ€² = [R(s, a) for s âˆˆ ð’®, a âˆˆ ð’œ]
    return ð’®â€², ð’œâ€², Râ€²
end
\end{lstlisting}

Finally, listing \ref{lst:lp-solving} shows how we can use the Julia packages
JuMP and GLPK to solve the specific Linear Program associated with the
\ac{alpbdt}. JuMP is a package that introduces a Domain Specific Language to
Julia that allows the user to use a mathematics-friendly notation to pose
optimization problems, without having to worry about solver-specific code or
representations. On the other hand, the GLPK.jl package is a wrapper of the
widely used, battle-tested GNU Linear Programming Kit, which is used to solve
Linear and Mixed Integer Programs.

\begin{lstlisting}[
    language=Julia,
    caption={Code used to solve a Linear Program},
    label={lst:lp-solving}
]
function ELP(p::DiscreteMDP)
    ð’®, ð’œ, R, T = tensorform(p)
    model = Model(GLPK.Optimizer)
    @variable(model, U[ð’®])
    @objective(model, Min, sum(U))
    @constraint(model, [s=ð’®, a=ð’œ], U[s] >= R[s, a] + p.Î³ * T[s, a, :] Â· U)
    optimize!(model)
    return ValueFunctionPolicy(p, value.(U))
end
\end{lstlisting}

Using the code presented this chapter, to provide a complete implementation of
\ac{alpbdt} we would need to provide an instance of the \lstinline{DiscreteMDP}
structure that describes the \ac{cart} fitting procedure within the confines of
\iac{mdp}. Even though the framing and justification are fully developed, this
thesis has avoided analysis of some of the other important parameters and
considerations that are crucial to a working implementation of \ac{cart}. Some
crucial concepts that would need to be introduced and developed further are:
\begin{itemize}
    \item Cross validation,
    \item \Ac{cart} hyper parameters and their interpretation,
    \item Possible evaluation metrics,
\end{itemize}
to name a few. Additionally, this thesis has introduced some other important
pieces that are exclusive to \ac{alpbdt} and thus not contained in the
literature that serves as guidance for a full implementation of \ac{cart}. Such
a task extends far beyond the scope of a bachelor's thesis. That being said, the
remaining tasks are greatly facilitated by having a theoretical background,
which is what this thesis aims to provide.

\section{Closing thoughts}

The code presented in this chapter aims to illustrate the concepts developed in
previous chapters and has served to test and gain intuition about classic
\ac{rl} problems. The code itself is based upon the code presented in
\cite{kochenderfer2022} and would be useful for solving the \ac{rl} problem via
Exact Linear Programming without modifications. 

Initially, this chapter aimed to provide a full implementation of \ac{alpbdt}
with the ultimate goal of comparing its performance against state-of-the-art
implementations of \ac{cart}. Namely, the implementations in MLJ.jl and the
\ac{alpbdt} algorithm presented by \citeauthor{xiong}. The \ac{rlbdt} algorithm
was of particular interest, since the authors claim a sizeable advantage on both
training cost and performance compared to \ac{cart}. For the reasons outlined
earlier a working implementation of \ac{alpbdt} could not be included, which
regrettably means that a direct comparison with \ac{rlbdt} cannot be made. For
more information on the claimed advantages of \ac{rlbdt} over cart, please refer
to \cite{xiong}.