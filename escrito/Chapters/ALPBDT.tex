\chapter{Approximate Linear Programming Based Decission Trees}
\label{chapter:ALPBDT.tex}

In chapter \ref{chapter:CARTasRLP} we showed how the \ac{cart} fitting procedure
can be cast an \ac{mdp} and consequently tackled as a Reinforcement Learning
Problem. We concluded that the state space $\mathcal{S}$ corresponds to the set
of all partitions of the training set $\L_t$; the action space coresponds to the
set of all observed values for each available feature, and the state-value
function is the Gini impurity index. In this chapter we will define the
necessary abstractions to implement the method we named \acf{alpbdt} in Julia.
We use Julia because it allows for an efficient implementation, ease of use, and
enjoys some of the best ecosystems for optimization.

To represent \iac{mdp}, we define a some types. Using the code in
\cite{kochenderfer2022} as inspiration, listing \ref{lst:structs} presents the
code needed to represent \iac{mdp}. The state and action spaces are represented
using Vectors, which in Julia are contiguous arrays. The probability transition
function is represented as a 3-dimensional matrix, since it aims to capture $p(s
\mid s', a)$. This is represented as a Union of a 3-d Matrix and Nothing to
allow for problems that have determinisitc transitions.

\begin{lstlisting}[
    language=Julia, caption={Type definition of an \ac{mdp}}, label={lst:structs}
    ]
struct State name::Symbol end

struct Action name::Symbol end

struct DiscreteMDP
    ğ’®   ::Vector{State}
    ğ’œ   ::Vector{Action}
    p   ::Union{Array{Float64, 3}, Nothing}
    R   ::Function
    Î³   ::Float64
end
\end{lstlisting}

In listing \ref{lst:basic-funcs} we define implement a lookahead function that
will be neede later to solve the RL problem, and the definition of a policy
function.

\begin{lstlisting}[
    language=Julia, caption={Basic \ac{mdp} functionality},
    label={lst:basic-funcs}
    ]
function lookahead(P::DiscreteMDP, U, s::State, a::Action)
    S, R, Î³ = P.ğ’®, P.R, P.Î³
    return R(s, a) + Î³ * sum(U[i] for (i, sâ€²) in enumerate(S))
end

struct ValueFunctionPolicy
    P   ::DiscreteMDP
    U   ::Function
end

function greedy(P::DiscreteMDP, U, s::State)
    u, a = findmax(a -> lookahead(P, U, s, a), P.ğ’œ)
    return (a=a, u=u)
end

(Ï€::ValueFunctionPolicy)(s::State) = Ï€.P.ğ’œ[greedy(Ï€.P, Ï€.U, s).a]
\end{lstlisting}

Listing \ref{lst:LP-form} deals with the Exact Linear Programming formulation
discussed in chapter \ref{chapter:ApproximateLinearP}.

\begin{lstlisting}[
    language=Julia,
    caption={Linear Programming formulation for an RL Problem},
    label={lst:LP-form}
]
function tensorform(P::DiscreteMDP)
    ğ’®, ğ’œ, R = P.ğ’®, P.ğ’œ, P.R
    ğ’®â€² = eachindex(ğ’®)
    ğ’œâ€² = eachindex(ğ’œ)
    Râ€² = [R(s, a) for s âˆˆ ğ’®, a âˆˆ ğ’œ]
    return ğ’®â€², ğ’œâ€², Râ€²
end
\end{lstlisting}