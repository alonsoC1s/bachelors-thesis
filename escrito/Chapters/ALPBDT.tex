\chapter{Approximate Linear Programming Based Decission Trees}
\label{chapter:ALPBDT}

In chapter \ref{chapter:CARTasRLP} we showed how the \ac{cart} fitting procedure
can be cast an \ac{mdp} and consequently tackled as a Reinforcement Learning
Problem. We concluded that the state space $\mathcal{S}$ corresponds to the set
of all partitions of the training set $\L_t$; the action space coresponds to the
set of all observed values for each available feature, and the state-value
function is the Gini impurity index. In this chapter we will define the
necessary abstractions to implement the method we named \acf{alpbdt} in Julia.
We use Julia because it allows for an efficient implementation, ease of use, and
enjoys some of the best ecosystems for optimization problems.

To represent \iac{mdp}, we define a some types. Using the code in
\cite{kochenderfer2022} as inspiration, listing \ref{lst:structs} presents the
code needed to represent \iac{mdp}. The state and action spaces are represented
using \lstinline{Vector}s, which in Julia are arrays allocated contiguously. The
probability transition function is represented as a 3-dimensional
\lstinline{Matrix}, since it aims to capture $p(s \mid s', a)$, a function of
three arguments. This is represented as a \lstinline{Union} of a 3-d
\lstinline{Matrix} and \lstinline{Nothing} to allow for problems that have
determinisitc transitions.

\begin{lstlisting}[
    language=Julia, caption={Type definition of an \ac{mdp}}, label={lst:structs}
    ]
struct State name::Symbol end

struct Action name::Symbol end

struct DiscreteMDP
    ð’®   ::Vector{State}
    ð’œ   ::Vector{Action}
    p   ::Union{Array{Float64, 3}, Nothing}
    R   ::Function
    Î³   ::Float64
end
\end{lstlisting}

In listing \ref{lst:basic-funcs} we define implement a lookahead function that
will be neede later to solve the RL problem, and the definition of a policy
function.

\begin{lstlisting}[
    language=Julia, caption={Basic \ac{mdp} functionality},
    label={lst:basic-funcs}
    ]
function lookahead(P::DiscreteMDP, U, s::State, a::Action)
    S, R, Î³ = P.ð’®, P.R, P.Î³
    return R(s, a) + Î³ * sum(U[i] for (i, sâ€²) âˆˆ enumerate(S))
end

struct ValueFunctionPolicy
    P   ::DiscreteMDP
    U   ::Function
end

function greedy(P::DiscreteMDP, U, s::State)
    u, a = findmax(a -> lookahead(P, U, s, a), P.ð’œ)
    return (a=a, u=u)
end

(Ï€::ValueFunctionPolicy)(s::State) = Ï€.P.ð’œ[greedy(Ï€.P, Ï€.U, s).a]
\end{lstlisting}

Listing \ref{lst:LP-form} deals with the Exact Linear Programming formulation
discussed in chapter \ref{chapter:ApproximateLinearP}.

Listing \ref{lst:LP-form} presents how the vectors and matrices needed to define
the Reduced Linear Program associated with the \ac{cart} fitting procedure
\eqref{lp:cart-elp} can be computed. These will be necessary later on to solve
the Linear Program. Julia's built-in function \lstinline{eachindex} returns an
iterable of efficient indexes that can be used to iterate over the structure
passed as its argument.

\begin{lstlisting}[
    language=Julia,
    caption={Linear Programming formulation for an RL Problem},
    label={lst:LP-form}
]
function tensorform(P::DiscreteMDP)
    ð’®, ð’œ, R = P.ð’®, P.ð’œ, P.R
    ð’®â€² = eachindex(ð’®)
    ð’œâ€² = eachindex(ð’œ)
    Râ€² = [R(s, a) for s âˆˆ ð’®, a âˆˆ ð’œ]
    return ð’®â€², ð’œâ€², Râ€²
end
\end{lstlisting}

Finally, listing \ref{lst:lp-solving} shows how we can use the Julia packages
JuMP and GLPK to solve the specific Linear Program associated with the
\ac{alpbdt}. JuMP is a package that introduces a Domain Specific Language to
Julia that allows the user to use a mathematics-friendly notation to pose
optimization problems, without having to worry about solver-specific code or
representations. On the other hand, the GLPK.jl package is a wrapper of the
widely used, battle-tested GNU Linear Programming Kit, which is used to solve
Linear and Mixed Integer Programs.

\begin{lstlisting}[
    language=Julia,
    caption={Code used to solve a Linear Program},
    label={lst:lp-solving}
]
function ELP(p::DiscreteMDP)
    ð’®, ð’œ, R, T = tensorform(p)
    model = Model(GLPK.Optimizer)
    @variable(model, U[ð’®])
    @objective(model, Min, sum(U))
    @constraint(model, [s=ð’®, a=ð’œ], U[s] >= R[s, a] + p.Î³ * T[s, a, :] Â· U)
    optimize!(model)
    return ValueFunctionPolicy(p, value.(U))
end
\end{lstlisting}

Using the code presented this chapter, all that is left to implement to finally
provide a complete implementation of \ac{alpbdt} is to provide an instance of
the \lstinline{DiscreteMDP} structure that captures the \ac{cart} fitting
procedure within the confines of \iac{mdp}. This is relatively simple to do once
we developed the theory of Chapter \ref{chapter:CARTasRLP}. Regretably, we do
not include that final step in the present work for several reasons. For
instance, to fully develop the remaining steps we would need to develop in full
the concepts of cross validation, and introduce the classic hyperparameters used
in the \ac{cart} fitting procedures such as max depth, and minimum samples per
leaf, to name a few. That being said, the theoretical groundwork is laid out in
this thesis. What remains to be done is an engineering problem.

Since the implementation is incomplete, we cannot compare the performance of the
algorithm proposed here with the figures presented by \citeauthor{xiong}.