
\label{chapter:ReinforcementLearning}

On Chapter~\ref{chapter:motivation} we presented a very simple 
example of a problem that may be solved via Reinforcement 
Learning: a robot trying to play a game we called Miniopoly. As 
simple as that example was, the key ideas will now allow us to 
delve into the theory proper, and formalize some ideas while 
hopefully giving satisfying answers to some questions that the 
intuitive treatment might have left open.

\section{The Agent \& the Environment}
Recall from Chapter~\ref{chapter:Motivation} that we referred 
to the robot player as a ``learning agent''. This agent is 
continuously interacting with the rest of the game and 
surveying its current state. As we saw earlier, the robot then 
selects an action on the basis of this current game state.

To be precise, say that the game starts at some $t=0$ and ends 
at $t=T$. We discretize this period of time into $t \in \{0, 1, 
2, \ldots \}$ At each of these points $t$ in time, the agent 
finds itself at some state $S_t \in \States$, where $\States$ 
represents the set of all possible states. This defines exactly 
what we called a stochastic process back in 
Chapter~\ref{chapter:Stochastic}: a series of random variables 
$\{ S_t \}_{t = 0, 1 \ldots}$. 

Likewise, at each time step $t$ the agent chooses how to 
interact with an action $A_t \in \Actions(s)$, where 
$\Actions(s)$ is the set of all \textit{available} actions at 
the current state $s$. This too forms a stochastic process. One 
time step in the future $t+1$ the agent receives more feedback 
from the environment, the so-called reward signal $R_{t+1} \in 
\Rewards \subseteq \R$. This process moves forward from $t$ to 
$t+1$ and so on until the task is over. This defines a Markov 
Decision Process (MDP from now on), which we can reorder as 
follows:
\begin{equation}
	S_0, A_0, R_1, A_1, R_2, S_2, A_2, R_3, \ldots
\end{equation}

This particular ordering makes it easy to see why we chose to 
discretize time into steps. This illustrates how at each time 
step the agent surveys the state, and based on it selects a 
\textit{feasible} action. The next time step begins when the 
robot receives a reward signal from the environment. 

A key idea is subtly hidden in the notation. Notice how only 
the set of actions depends on a particular $s$. Both the set of 
states $\States$ and the set of rewards $\Rewards$ are in a 
certain sense independent of $s$. Why is it that only 
$\Actions(s)$ is dependent on $s$?

The ``Markov'' in Markov Decision Process is exactly what makes 
the set of actions special. As covered in 
Chapter~\ref{chapter:Stochastic}, a Markov process is often 
used to describe stochastic transitions between a set 
of---emphasis on terminology here---\textit{states}. In the 
example on Chapter~\ref{chapter:Motivation} the states were the 
literal squares in the game. And as we discussed, not every 
state is accessible from any other state. Thus, it makes sense 
that for each state the set of possible actions is determined 
by it. Some transitions are just impossible.

Not only are some transitions impossible at certain states, a 
given action chosen by the agent will not always result in the 
same transition from the state $s$ to the state $s'$, which 
might sound obvious as we have already established that this 
entire process can be best modeled by a MDP. To study the 
probability of a certain transition from state $s$ to state 
$s'$ given that a certain action $a \in \Actions(s)$ was taken 
we introduce the \textit{dynamics} function, keeping with the 
notation in \cite{SuttonBarto}.

\begin{dfn}{Dynamics function}{dynamics-func}
	For all $s, s' \in \States, r \in \Rewards, a \in 
	\Actions(s)$, we define the \emph{dynamics function} $f: 
	\States \times \Rewards \times \States \times \Actions(s) 
	\to [0, 1]$ as follows:
	\begin{equation*}
		p(s', r \mid s, a) \coloneqq \IP{S_t = s', R_t = r 
		\vertsep S_{t-1} = s, A_{t-1} = a}.
	\end{equation*}
\end{dfn}

Hola \ref{dfn:dynamics-func}
