\chapter{Approximate Linear Programming}
\label{chapter:ApproximateLinearP}
\section{Introduction}

As discussed in chapter \ref{chapter:ReinforcementLearning}, the problem of
finding the best policies by using Bellman's optimality equations falls within
the realm of dynamic programming. The problem is that even if an explicit
solution can be given under certain conditions, the computational burden of
calculating exact solutions is often impossible to overcome, even on modern
computing equipment. And given the ``curse of dimensionality'', even if the
contemporary problems became tractable in the future thanks to the
ever-increasing computing power, that very same improvement in computing power
would motivate researchers to tackle bigger still problems. So, to use the
\ac{rl} techniques, we must find a way to use our computational resources more
efficiently. 

Since Reinforcement Learning happens to be part of the techniques often grouped
under the umbrella of ``artificial intelligence'', it has enjoyed much attention
for decades. Thanks to these research efforts and numerous applications in the
industry, there are several battle-tested approximate solutions to the \ac{rl}
problem we have developed here. Among these are: Q-learning, Monte--Carlo
estimation methods, temporal difference learning, and many others that are
described in detail in \cite[Chapter~4]{SuttonBarto}. The somewhat novel
technique described in this thesis was developed in the first decade of the
2000s and is not part of the standard toolbox for solving \ac{rl} problems since
it was developed initially in the area of management science and particularly
for the problem of probabilistic inventory management, following previous work,
laid out since the 1960s. 

Specifically, the technique to be laid out in this part of the thesis was
developed by \citeauthor*{farias2003LP2ADP}, as a continuation of previous work
by \citeauthor*{denardo1970} and \citeauthor*{depenoux1963} in
\cite{denardo1970} and \cite{depenoux1963}, respectively. In a nutshell,
\citeauthor*{farias2002thesis} casts the dynamic programming problem that arises
from solving Bellman's optimality equations as a linear program and then gets
around the curse of dimensionality by using linear approximations for the
interest functions to reduce the number of variables in the problem. Without
further ado, let us get to the details right after developing the necessary
background.

\section{Exact Dynamic Programming}

In Chapter \ref{chapter:ReinforcementLearning}, we showed that using Bellman's
optimality equations, we can obtain optimal policies if we have access to the
optimal value ($v_*$) or action-value ($q_{*}$) functions. We denote these
functions subindexed by $*$ to accentuate the fact that they are optimal in the
sense that they satisfy Bellman's optimality equations, which are
\eqref{eq:bellmans-value} and \eqref{eq:bellmans-action-value} for $v_*$ and
$q_*$ respectively. 
\begin{align}
v_{*}(s) &= \max_{a} \sum_{s', r} p(s', r \mid s, a) \left[ r + \gamma v_{*} (s')
\right], \label{eq:bellmans-value} \\
q_{*}(s, a) &= \sum_{s', r} p(s', r \mid s, a) \left[ r + \gamma \max_{a'} q_{*}
(s', a') \right], \label{eq:bellmans-action-value}
\end{align}
for all $s \in \States, a \in \Actions$.

Equation \eqref{eq:bellmans-value} is important, but not that helpful in
practice. It tells us the best possible reward a learning agent can expect in a
learning task. In other words, the best possible reward \textit{after} the agent
followed the best policy but tells us very little about how to obtain that
policy. We would like to predict the total reward a given policy will yield.
Thankfully, in chapter \ref{chapter:ReinforcementLearning}, we obtained an
expression to calculate the expected reward a given policy $\pi$ will yield
starting from a certain state $s$, that we will refer to as \textit{Bellman's
recurrence equation} from now on: 
\begin{equation}
\label{eq:bellmans-recurrence}
% TODO: Transcribir (4.4) de S&B.
v_\pi (s) = \sum_{a \in \Actions} \pi(a \mid s) \sum_{s', r} p(s', r \mid s, a) \left[ r + \gamma v_{\pi}(s') \right].
\end{equation}

The identity \eqref{eq:bellmans-recurrence} was proved earlier in chapter
\ref{chapter:ReinforcementLearning}, Lemma \ref{lem:recursive-eval}: the
Recursive Evaluation Lemma. We named the Lemma that way for reasons that will
become apparent soon.

Using \eqref{eq:bellmans-recurrence} we can obtain an exact solution for $v_\pi$
solving one equation for each state $s \in \States$. This is often called the
\textit{prediction problem} in the literature \cite[Chapter~4.1]{SuttonBarto}.
Transforming equation \eqref{eq:bellmans-recurrence} we obtain the following,
simpler expression:
\begin{align}
\label{eq:bellmans-recurrence-prime}
v_\pi(s) &= \sum_{a} \pi(a \mid s) \sum_{s', r} p(s', r \mid s, a) \left[ r + \gamma v_\pi (s') \right] \nonumber \\
&= \sum_{a} \pi(a \mid s) \left[ \sum_{s', r} r p(s', r \mid s, a) + \gamma \sum_{s', r} v_\pi (s') p(s', r \mid s,a) \right] \nonumber \\
&= \sum_a \pi(a \mid s) \underbrace{\sum_{r} r \sum_{s'} p(s', r \mid s, a)}_{r(s,a) \text{(by dfn. \ref{dfn:rewards-func})}} +
    \gamma \sum_{a} \pi(a \mid s) \sum_{s', r} v_\pi (s') p(s', r \mid s, a) \nonumber \\
&= \sum_{a} \pi(a \mid s) \left[ r(s,a) + \gamma \sum_{s'} p(s' \mid s, a) v_\pi (s') \right] \tag{\ref*{eq:bellmans-recurrence}$\prime$}.
\end{align}

Since \eqref{eq:bellmans-recurrence-prime} defines one equation for each state,
we can think of the system of linear equations defined in terms of vectors and
matrices. This approach will later serve to define Bellman's policy\footnote{or
``one-step'' \cite[pg.~9]{nadeemward2021}, or ``expectation backup''
\cite[Lect.~3, Contraction Mapping]{silver2015}, the literature uses various
names. We follow \cite{raoRL4F}.} and optimality operators, a powerful tool.

For now, let us consider the value function as a vector, following
\cite[pg.~132]{raoRL4F}. Specifically,
\[
    \vec{v} = \left[ v (s_1), \dots , v (s_{|\States|}) \right].
\]
As shorthand notation we define $\vec{v}(s) \coloneqq v(s)$. Recall that $r(s,
a)$ is the expected reward upon taking action~$a$ being in state~$s$, and $p(s'
\mid s, a)$ is the probability of transitioning directly from state~$s$ to
state~$s'$ after taking action~$a$. We define:
\begin{align*}
    R_\pi (s) &= \sum_{a \in \Actions} \pi(a \mid s) \, r(s,a), \\
    P_\pi (s, s') &= \sum_{a \in \Actions} \pi(a \mid s) \sum_{s' \in \States} p(s' \mid s, a),
\end{align*}
which result from a slightly rearranged version of
\eqref{eq:bellmans-recurrence-prime}.

We denote by $\vec{R}_\pi$ the vector $\left[ R_\pi(s_1), \dots, R_\pi
(s_{|\States|}) \right]$ and $\vec{P}_\pi$ the stochastic matrix $\left[
P_\pi(s_i, s_j) \right]$ that defines the transition probabilities from any
state $s_i$ to every other distinct state $s_j$ with $i, j \in \left\{ 1, \dots,
|\States| \right\}$. As promised, this is the key to what will become one of our
most powerful tools: Bellman's policy operator.  This will enable us to study
how \textit{any} $v$ acts on the set of states $\States$.

\begin{dfn}{Bellman's Policy Operator}{bellmans-pol-operator}
    We denote by $T_\pi$ the operator $T_\pi: \R^{|\States|} \to \R^{|\States|}$
    defined as:
    \[
        T_\pi \vec{v} = \vec{R}_\pi + \gamma \vec{P}_\pi \vec{v},
    \]
    for any value function vector $\vec{v} \in \R^{|\States|}$. The definition
    of this operator can be found in \cite[Ch.~5.4]{raoRL4F}.
\end{dfn}

Using the newly defined Bellman's Policy Operator, we can rewrite Bellman's
recurrence equation \eqref{eq:bellmans-recurrence} as
\begin{equation}
    \label{eq:BPO-fixed-point}
    T_\pi \vec{v}_\pi = \vec{v}_\pi.
\end{equation}
This means $\vec{v}_\pi$ is a fixed point of the Bellman Policy Operator.

Using the same arguments that led to \eqref{eq:bellmans-recurrence-prime}, let
us write Bellman's optimality equation for the value function
\eqref{eq:bellmans-value} using another operator.

\begin{dfn}{Bellman's Optimality Operator}{bellmans-opt-operator}
    We denote by $T_*: \R^{|\States|} \to \R^{|\States|}$ \emph{Bellman's Optimality Operator}, defined for each $s \in \States$ as:
    % FIXME: Aqui change r(s,a) va multiplicado con vector de 1's.
    \[
        (T_{*} \vec{v})(s) = \max_{a \in \Actions} \left\{ r(s, a) + \gamma \sum_{s' \in \States} p(s' \mid s, a) v(s') \right\}. 
    \]
\end{dfn}

Using the Optimality Operator, \eqref{eq:bellmans-value} can be rewritten as
\begin{equation}
    \label{eq:bellmans-optimality-operators}
    T_* \vec{v}_{*} = \vec{v}_{*}.
\end{equation}

% In other words, the optimality operator $T_*$ is a non-linear operator with a
% fixed point $\vec{v}_*$ \cite[Ch.~5.4]{raoRL4F}.

Theorem \ref{thrm:farias2002thesis-thrm2.1} (corresponding to Theorem 2.1 in
\cite[Ch.~2.2]{farias2002thesis}) presents some key properties of Bellman's
Policy Operator. We are especially interested in the fact that it has a unique
fixed-point.

\begin{thrm}{}{farias2002thesis-thrm2.1}
    Let $\vec{v}$ be an arbitrary value function. Then,
    \begin{enumerate}
        \item $\left\| T\vec{v} - T \vec{v}_* \right\|_{\infty} \leq \gamma
            \left\| \vec{v} - \vec{v}_* \right\|$.
        \item If $\vec{v}_0 \geq \vec{v}_1$, then $T\vec{v}_0 \geq T\vec{v}_1$.
        \item The operator $T$ has a unique fixed-point given by $\vec{v}_*$.
    \end{enumerate}
\end{thrm}

The equation presented in \eqref{eq:BPO-fixed-point} is equivalent to statement
3 on the previous theorem. As promised, Bellman's operators yield the solutions
to both the prediction problem and the optimal value function, which come in
handy to prove whether or not our \ac{rl} problem has solutions or under which
circumstances. However, so far, we have no idea how to solve them.

With our toolbox almost complete, it is time to advance our search for
optimality. As previously mentioned, there are several approaches to solving
Bellman's equations (\eqref{eq:bellmans-value} and
\eqref{eq:bellmans-action-value}) are based on dynamic programming, yielding
several algorithms we do not review in detail in this thesis as they are outside
of scope. The literature for those techniques is excellent. If interested in a
more detailed treatment of said algorithms, please review \cite{SuttonBarto} and
\cite{raoRL4F}.

\subsection{Approaching optimality}

So far, our goal has been to find the ``best'' policy, but what does it mean for
a policy to be the best? We can not compare policies directly, but we can
compare the value function's value for each of them. The optimal, the
``best'' policy is the one that maximizes the value function.

\begin{dfn}{Optimal Policy}{optimal-policy}
    We say that a given policy $\pi$ is in a certain sense \textit{better} than
    the policy $\pi'$, which we write as $\pi \geq \pi'$, whenever $v_\pi(s)
    \geq v_{\pi'} (s)$ for every $s \in \States$. This is called a partial
    ordering over the space of policies. A policy $\pi_*$ is optimal if
    $\pi_*(s) \geq \pi (s)$ for all $s$ and all other policies $\pi$.
\end{dfn}

The equality in definition \ref{dfn:optimal-policy} is only satisfied when both
policies are optimal, as stated in the following Lemma which correspondes to
Lemma 4.10.2 in \cite[pg.~115]{raoRL4F}.

\begin{lemma}{}{equality-on-optimality}
    For any two optimal policies $\pi^{*}_{1}$ and $\pi^{*}_{2}$, for all $s \in
    \States$ they evaluate to the same on the value function, that is,
    $v_{\pi^{*}_{1}} (s) = v_{\pi^{*}_{2}} (s)$.  Using the vector notation
    introduced earlier, $\vec{v}_{\pi_{1}^{*}} = \vec{v}_{\pi_{2}^{*}}$.
\end{lemma}

\begin{proof}
    Since $\pi_{1}^{*}$ is an optimal policy, from the optimal policy definition
    we have: $v_{\pi_{1}^{*}}(s) \geq v_{\pi_{2}^{*}} (s)$ for all $s$.
    Likewise, since $\pi_{2}^{*}$ is also an optimal policy, $v_{\pi_{2}^{*}}(s)
    \geq v_{\pi_{1}^{*}} (s)$ for all $s$. Therefore, $v_{\pi_{1}^{*}}(s) =
    v_{\pi_{2}^{*}} (s)$ for all $s$.
\end{proof}

Wielding Lemma \ref{lem:equality-on-optimality} we can prove that there always
exists an optimal policy for the \ac{rl} problem we have been studying.

\begin{thrm}{}{opt-policy-existence}
    For a Reinforcement Problem based on a discrete-time, finite-space Markov
    Decision Process, the following hold:
    \begin{itemize}
        \item There exists an optimal policy $\pi_*$. That is, $v_{\pi_*} (s)
            \geq v_{\pi}(s)$ for all other policies $\pi$ and all states $s \in
            \States$.
        \item All optimal policies achieve the optimal value function given by
            \eqref{eq:bellmans-value}. That is, $v_{\pi_*}(s) = v_* (s)$ for all
            $s \in \States$ where $\pi_*$ is one optimal policy.
        \item All optimal policies achieve the optimal action-value function,
            given by \eqref{eq:bellmans-action-value}.
    \end{itemize}
\end{thrm}

\begin{proof}
    We follow the proof in \cite[pg.~115]{raoRL4F} closely.

    As a consequence of Lemma \ref{lem:equality-on-optimality}, we only need to
    find a policy that maximizes both the value and action value functions,
    achieving the maximum values: $v_*$ and $q_*$ respectively.

    A policy that is a candidate to be optimal can be constructed as follows:
    \begin{equation}
        \label{eq:greedy-policy-construction}
        \pi_{*}^{D} (s) = \argmax_{a} q_{*} (s, a) \quad \forall s \in \States.
    \end{equation}

    For now, we assume that the resulting action $a$ for any given state $s$ is
    unique.

    We show that $\pi_{*}^{D}$ maximizes the optimal value and action-value
    functions. As established in chapter \ref{chapter:ReinforcementLearning},
    equation \eqref{eq:prop-dependencia-vq}, $v_* (s) = \max_a q_* (s, a)$, and
    therefore,
    \[
        v_* (s) = q_* \left(s, \pi_{*}^{D}(s) \right).
    \]

    In other words, we maximize the value function if we first take the action
    prescribed by the policy $\pi_{*}^{D}$ followed by the action from the next
    time steps that maximize the value function, and so on. This is precisely
    why Bellman's recurrence equations work as they do. Likewise, since the
    state-value and action-value can be expressed in terms of each other as
    reviewed in chapter \ref{chapter:ReinforcementLearning} the action-value
    function is maximized following whatever action $\pi_{*}^{D}$ prescribes.
    Formally,
    \[
        q_{\pi_{*}^{D}} (s, a) = q_* (s,a) \quad \forall s \in \States, a \in \Actions.
    \]

    Lastly, we show that $\pi_{*}^{D}$ is an optimal policy by contradiction.

    Suppose that $\pi_{*}^{D}$ is not an optimal policy. That means that there
    exists some other policy $\pi$ such that $\pi > \pi_{*}^{D}$. In the partial
    order defined previously, that means that $v_{\pi} (s) > v_{\pi_{*}^{D}}
    (s)$ for at least one state $s$! This contradicts the definition of the
    optimal value function $v_* (s) = \max_\pi v_\pi (s)$ for all $s$. Therefore
    $\pi_{*}^{D}$ must be an optimal policy.
\end{proof}

Notice how the optimal policy was constructed by prescribing to the state $s$
whichever action maximizes the value function at that state. In other words, the
policy constructed was \emph{greedy}. A much more exciting and aesthetically
pleasing way of proving theorem \ref{thrm:opt-policy-existence} is by using the
properties of Bellman's operators. Theorem 5.4.1 in \cite[pg.~130]{raoRL4F},
reproduced below, allows us to conclude similar things to Theorem
\ref{thrm:opt-policy-existence}.

\begin{thrm}{Policy Evaluation Convergence Theorem}{pect}
    The value function $\vec{v}_\pi$ is the \underline{unique} fixed-point of
    the Bellman Policy Operator $T_\pi$, and
    \[
        \lim_{t \to \infty} (T_\pi)^{i} \vec{v}_0  \to \vec{v}_\pi,
    \]
    for any starting $\vec{v}_0$.
\end{thrm}

As exemplified by the gradient descent operator presented in chapter
\ref{chapter:OptimizationTheory}, once again the solution to an optimization
problem reduces to finding fixed points of an operator over a specific space.
The proof of Theorem \ref{thrm:pect} can be found in \cite[pg.~131]{raoRL4F}. We
do not include the proof it it requieres a considerable amount of preliminaries
and is extensive. Nonetheless, we think it is a very pleasing application of
Banach's Fixed-Point Theorem.

Theorem \ref{thrm:opt-policy-existence} guarantees the existence of an optimal
policy in theory. But we are interested in finding such a policy in practice,
preferably in a way that is computable this century. Luckily, Theorem
\ref{thrm:pect} provides an update rule that converges to the stationary point
of Bellman's Policy Operator and thus the optimal value function. That approach
is called \emph{value iteration} and is described in detail in
\cite{SuttonBarto,raoRL4F}, we focus on other approaches.

\section{Approximating solutions with Linear Programming}

Having established the necessary context, we can at last address the main
objective of this Chapter, and indeed the whole thesis: avoiding the use of
inefficient dynamic programming and use linear programming (LP) to solve
Bellman's equations. We reproduce Theorem 6.2.2 in
\cite[Ch.~6.9.1]{puterman2014}, adapted to our notation, as this is the basis
for the LP formulation.

\begin{thrm}{}{6.2.2-puterman}
    Suppose there exists a value function $\vec{v}$ for which
    \begin{enumerate}
        \item $\vec{v} \geq T_{*} \vec{v}$, then $\vec{v} \geq \vec{v}_*$.
        \item $\vec{v} \leq T_{*} \vec{v}$, then $\vec{v} \leq \vec{v}_*$.
    \end{enumerate}
\end{thrm}

By Theorem \ref{thrm:6.2.2-puterman}, statement 2, whenever a given policy
$\vec{v}$ satisfies
\[
    \vec{v} \leq T_* \vec{v}
\]
which then, by Bellman's optimality equation
\eqref{eq:bellmans-optimality-operators} gives
\[
    \vec{v} \leq \vec{v}_* = T_* \vec{v}_*,
\]
where vectors $\vec{v}$ and $\vec{v}_*$ are compared component-wise. In other
words, we can try to find a lower bound $\vec{v} \leq T_{*} \vec{v}$ by
considering Definition \ref{dfn:bellmans-opt-operator} which gives the following
set of linear constraints:
\[
    v(s) \leq \max_{a} \left\{ r(s, a) + \gamma \sum_{s'} p(s' \mid s, a) \, v(s') \right\} \quad \forall s \in \States.
\]
In particular
\begin{equation}
    \label{eq:upper-bound-v}
    v(s) \leq r(s, a) + \gamma \sum_{s'} p(s' \mid s, a) \, v(s') \quad \forall s \in \States, a \in \Actions.
\end{equation}

Since the solution to \eqref{eq:bellmans-optimality-operators} is guaranteed to
exist by Theorem \ref{thrm:opt-policy-existence} and satisfy: $\vec{v}_* = T_*
\vec{v}_*$, we can approach it by finding the biggest vector upper bound. We
can generate lower bounds to the optimal value function by using
\eqref{eq:upper-bound-v}.

Finding the biggest upper bound leads to the following optimization problem:
\begin{equation}
\begin{array}{rl@{}ll}
    \displaystyle \max_{v: \States \to \R} & v(s) \\
    \text{Subject to} & \displaystyle v(s) \leq r(s,a) + \gamma \sum_{s'} p(s' \mid s, a) \, v(s') & \quad \forall a \in \Actions, s \in \States. \\
    & v(s) \text{ unconstrained,} & \quad \forall s \in \States.
\end{array}
\end{equation}

Making use of the vector notation introduced earlier, we can find a Linear
Program formulation to the previous optimization problem
\begin{equation}
\label{lp:exact-lp}
\tag{ELP}
\begin{array}{rl@{}ll}
    \displaystyle \max_{\vec{v} \in \R^{|\States|}} & \vec{c}^{\top} \vec{v} \\
    \text{Subject to} & \vec{v} \leq \vec{R}_a + \gamma \vec{P}_a \, \vec{v} & \quad \forall a \in \Actions \\
    & \vec{v} \text{ unconstrained.} &
\end{array}
\end{equation}
With $\vec{R}_a \in \R^{|\States|}$ defined as $[r(s_1, a), r(s_2, a), \dots,
r(s_{|\States|}, a)]^{\top}$ and $\vec{P}_{i,j}^{a} = p(s_j \mid s_i, a)$.

Professor \citeauthor{farias2002thesis} refers to Linear Program
\eqref{lp:exact-lp} as the \textit{exact Linear Program}
\cite[Ch.~2.3]{farias2002thesis}. This LP (linear program) has $|\States|$
variables and $|\States| \times |\Actions|$ constraints. This makes it
especially vulnerable to the curse of dimensionality, since we have as many
variables to optimize as states. Solving this LP for the number of states in a
modern \ac{rl} problem becomes prohibitively computationally expensive, even
when the average-case complexity of solving an LP is polynomial
\cite[pg.~147]{kochenderfer2022}, much better than listing the $|\States|!$
states.

To overcome this prohibitive cost, we turn to one of math's most proliferous
ideas: linear approximations. By approximating the function $v$ with a linear
basis we hope to reduce the number of variables, but the number of constraints
will remain the same. Chapter \ref{chapter:PropertiesGuarantees} proposes a way
to reduce the number of constraints.

\subsection{Linear everything}

Approximating complicated functions by collections of linear or affine functions
is not new. One of the most proliferous applications of this idea is linear
regression in the area of statistical learning, or its more glamorous name:
machine learning. The main idea behind linear regression is approximating a
relationship between a vector of \textit{regressors}, $\vec{x}_i$, measured
attributes observed for some phenomenon of interest; and a vector of response
variables $\vec{y}$. Specifically, the relationship to be estimated is the
conditional expectation of $\vec{y}$ given the $\vec{x}_i$, or $\mathbb{E}
\left[ \vec{y} \mid \vec{x}_i \right]$. In a nutshell, the relationship is
modeled by a linear function $\vec{x}_{i}^{\top} \vec{B}$. The matrix $\vec{B}$
is filled with coefficients that minimize the distance between the approximation
and the actual observed values $\vec{x}_i$. The key idea is that these
parameters are obtained by measuring the error between the predicted linear
relationship and the true, observed values. The exact nature of this process is
not important at the moment, but it is important to keep in mind that it can
only be carried out if we have access to the actual observed values of interest
$(\vec{x}_i, y_i)$. For our problem of interest, we have no such luxury. We have
to be a bit more clever.

\subsubsection{Linear architecture}
The strategy followed in \cite{farias2002thesis} is generating scoring functions
with a pa\-ra\-me\-trized class of functions, similar to linear regression but
carrying out the ``fitting'' without being able to sample the function we are
trying to approximate.

This parameterized class of functions will be a basis for the space of value
functions made up of linear functions $\varphi_i : \States \to \R$ for $i = 1,
\dots, K$, with $K \ll |\States|$ a set parameter. Following the standard
notation used in statistics, we denote $\widehat{\vec{v}} \approx \vec{v}$ the
linear approximation to the value function.

\begin{dfn}{Basis matrix}{}
    We define a matrix $\Phi \in \R^{|\States| \times K}$ given by:
    \[
        \Phi =
        \begin{bmatrix}
            | & \vdots & | \\
            \varphi_1 & \cdots & \varphi_K \\
            | & \vdots & |
        \end{bmatrix}.
    \]
\end{dfn}

This way, use the representation for $\widehat{\vec{v}} = \Phi \vec{\beta}$,
where $\vec{\beta}$ is a vector of parameters that will fit this representation.
Armed with this one last tool, let us take another look at the exact LP
\eqref{lp:exact-lp}, substituting the function for the approximation wherever
pertinent:
\begin{equation}
\label{lp:approx-lp}
\tag{ALP}
\begin{array}{rl@{}ll}
    \displaystyle \max_{\vec{\beta} \in \R^{K}} & \vec{c}^{\top} \Phi \vec{\beta} \\
    \text{S.t.} & \displaystyle \Phi \vec{\beta} \leq \vec{R}_a + \gamma \vec{P}_a \, \Phi \vec{\beta} & \quad \forall a \in \Actions, \\
    & \vec{\beta} \text{ unconstrained}.
\end{array}
\end{equation}

Once having an approximately optimal value function we can generate optimal
decisions according to:
\begin{equation}
    \label{eq:optimal-policy-generator}
    u_{\Phi \beta} (s) = \argmin_{a \in \mathcal{A}(s)} \left( \vec{R}_a + \gamma \vec{P}_a \Phi \beta \right),
\end{equation}
where the policy $\pi$ is subscripted by $\Phi\beta$ to emphasize how it is
generated. Notice that both sides of equation
\eqref{eq:optimal-policy-generator} depend on $s$. The right-hand side depends
on $s$ since the minimization is carried out over $\mathcal{A}(s)$, the set of
available actions given the current state.

The LP in \eqref{lp:approx-lp} is referred to as the \textit{approximate linear
program}. The vector $\vec{c}$ with positive components is called the
\emph{state-relevance weights} vector and determines, in a sense, the role each
basis function plays in approximating a certain characteristic of the target
function. Notice that the number of variables in this LP reduced from
$|\States|$ to $K$. The number of constraints was not reduced, but according to
\cite{farias2002thesis}, most of them become inactive, and solutions can be
approximated efficiently with modern methods. The rest of
\citeauthor{farias2002thesis}'s work shows how the special structure inherited
from dynamic programming can be used to efficiently sample constraints, making
the solution of this LP more efficient.

The remaining chapter of this part is dedicated to briefly reviewing the
extensions and results of this method.  Chapter
\ref{chapter:PropertiesGuarantees} continues the exploration of the Approximate
Linear Programming (ALP) method we have just described. In particular, some
desirable properties the optimization problem \eqref{lp:approx-lp} has and some
performance guarantees we can expect.


\section{Bibliographic Notes}

For the development of Bellman's Optimality Operator, we follow several sources:
\begin{itemize}
    \item Bellman's original paper \cite{bellman1957}.
    \item Several lectures and corresponding notes:
    \begin{itemize}
        \item David Silver's Course \cite[Lects.~2-3]{silver2015}.
        \item Basic Reinforcement Learning course at McGill University
            \cite[Lect.~2]{moisescu-parejaa}.
        \item Foundations of Reinforcement Learning with Applications in Finance
            course textbook by Ashwin Rao \cite[Ch.1-4]{raoRL4F}.
    \end{itemize}
    \item \citeauthor{nadeemward2021}'s thesis \cite{nadeemward2021} which
        summarizes the numerical approaches based on dynamic programming skipped
        in this chapter.
    \end{itemize}

For a more thorough development of why Bellman's Operator is defined, please
consult any of the sources referenced directly above.

The conceptual leap involved in casting the dynamic programming problem as a
linear program using the properties of Bellman's operators, entire chapters are
dedicated to explaining the background and motivating the ideas, and laying a
solid foundation in \cite{puterman2014}, specifically chapters~2 and 6.

Sadly, we are constrained in scope and can not possibly lay the entire
groundwork for this idea to feel more natural and better motivated. For the
interested reader, \citeauthor{puterman2014}'s book is a good--if involved--
read and is cited by several other bibliographic pieces used in this chapter.