\label{chapter:Motivation}
\section{A ``hand-wavy'' first approach}

What we refer to as Artificial Intelligence is a broad 
collection of methods and techniques used to solve a variety
of problems that we collectively associate with ``human 
intelligence'', such as identifying and classifying images, 
processing natural language, and learning from data.  Roughly 
speaking, the discipline can be divided into the following 
sub-fields of research:

\begin{itemize}
	\item Neural Networks.
	\item Computer vision.
	\item Natural Language Processing.
	\item Speech processing.
	\item Machine Learning.
\end{itemize}

This work is concerned specifically with Machine Learning.  
Particularly with a subset of the field known as Reinforcement 
Learning. Some authors such as \cite{SuttonBarto} divide the 
field of Machine Learning (ML) in three parts: supervised 
learning, unsupervised learning, and reinforcement learning.  
Both supervised and unsupervised learning rely heavily on 
statistics-based techniques such as regression models and 
statistical inference. The techniques used in reinforcement 
learning (RL), rely on different models of what learning is, 
and tries to go beyond finding association rules for a simple, 
well-defined problem.

In contrast to supervised learning where a learning agent is 
given data labeled by a knowledgeable source and must ``learn'' 
to classify based on those initial labels, in unsupervised 
learning as well as reinforcement learning, there is no 
``training data'', the agent must act and optimize its strategy 
based only upon the reward or penalty resulting from making a 
certain decision. The key words here are \textit{optimize}, 
\textit{strategy} and \textit{decision}.

For a concrete example picture an android at a casino playing 
at a slot machine, only this machine has $k$ levers instead of 
just one.  This robot is instructed to win as much money as 
possible across some number of games. How could this be 
achieved? Surely each lever makes the machine operate 
differently and thus is more or less likely to get a jackpot. 
If this android were a person, the approach would probably be 
to, across many games, try each lever and see how much money 
comes out. Hopefully, after the first few games this human 
player has a clear idea of which are the best levers to pull. 
This too might be a good strategy for the robot to employ, so 
lets explore that idea further.

If you think about it, the slot machine analogy is not so 
different from the standard mental model we have of say an 
infant learning to crawl.  With poor vision and only its 
caretaker's voice as guide, it must learn to find its way to 
safety by trial and error. It cannot be given millions of 
examples of ``valid paths'' for extrapolation, as is the case 
with supervised learning. This agent learns by interacting with 
the environment itself.

In a certain sense RL leverages our intuitions about the nature 
of learning. All the main elements are there: cause and effect, 
goals, and consequences to decisions made, but as we will see, 
it also encodes more subtle concepts. For instance, delayed 
gratification and planning ahead. It also has the novelty of 
being goal-oriented rather than task-oriented as most Machine 
Learning techniques often are. For example, a self-driving car, 
``trained'' via supervised learning might train on millions of 
examples on what constitutes a valid steering wheel move, is
expected to extrapolate to situations outside the training 
data. Meanwhile a self-driving car, learning by being evaluated 
is learning how to drive as an activity consisting of hundreds 
of little tasks, all to be mastered simultaneously.

\section{Formalizing ideas}
Now, mathematically speaking, what does it \textit{mean} for a 
machine to ``learn''?

We might not get as far as what learning as a whole means for 
humans or machines, but we can certainly discuss what 
mechanisms allow self-driving cars to exist or computers to 
beat world champions of Chess and Go. For that, we need to 
identify the key concepts in the picture I painted and express 
them in the language of mathematics.

For now, we can gloss over the details of how might a machine 
make decisions, perceive goals, take actions or perceive 
rewards. Let us focus on one key component: the ``learning''. 
When we talk about the process of a machine learning something 
we are thinking about something or someone (referred to as 
agent henceforth) that is able to keep track of the decisions 
it made before and whether or not they resulted in positive 
results so it can later on apply that knowledge to become an 
increasingly better problem solver. If this agent is evaluated 
each time it carries out some task, we would like for its score 
to increase each successive time it tries to complete the task. 
The idea of a continuously improving score is the foundation of 
mathematical optimization.

\subsection{Optimization Theory}
As the name suggests, mathematical optimization is concerned 
with finding an ``optimal'' (whatever ``optimal'' means) 
solution to problems where an unambiguous score might be given 
to different solutions. Even if there is no best solution to a 
given problem, the techniques used in mathematical optimization 
often allow for a continuous improvement through iteration.

Going back to our slot machine example, each time the robot 
selects one of the $k$ levers the machine gives out a prize. 
The prizes vary greatly in size, and there is no way to know 
which ones give the biggest prizes in advance. Since the robot 
wants to maximize money earned, if one lever is consistently 
giving out bigger prizes it is likely to keep pulling it. The 
robot was rewarded earlier for choosing that lever by getting a 
big prize.  Similarly if a lever pull resulted in no prize at 
all, the robot lost money by playing and probably will not 
choose that lever again. In each iteration the player robot 
would like to increase more and more the prize, and is thus  
encouraged to keep track of which levers give good prizes, 
improving the average prize in each successive game.

We are now ready to peel away another layer of abstraction.  We 
now have an intuition of what it means for a machine to learn, 
or in other words continually improve. But in the model we 
discussed earlier, the player is making choices along the way. 
A predefined path is not programmed or even known. If it were 
known this would not be a problem at all, the robot could 
choose the known-good levers every single time. Since that is 
not the case the robot must choose and balance between 
exploring and exploiting the knowledge gained so far. What does 
it mean for this robot player to ``make choices''?

\subsection{Stochasticity}
Another key aspect we take for granted when we talk about 
machines learning is implicit in the word learning. If there 
were a predefined path for the robot to take we would hardly 
call that learning, it is merely reproducing instructions.  
What this means for us, is that we must allow for a framework 
in which our robots actions are not completely determined 
beforehand. In fancier words, the succession of events that 
determine the robots actions and responses to stimuli are not 
predetermined or \textit{deterministic}.

This idea is formalized through something called a 
\textit{stochastic process}. The word stochastic is just 
mathematical lingo for ``not entirely determined beforehand'' 
or ``aptly described as a random phenomenon''.  In particular, 
we can think of the robot as being in some state among quite a 
few possible. The robot can change states as time goes by but 
the transitions are not always the same and they do not 
necessarily carry the same reward or punishment.

For instance, the robot at the slot machine might have gotten a 
very big prize the first time it chose a certain lever but got 
almost nothing the next game. The feedback it got by choosing 
an action changes as time goes by, because the slot machine 
behaves randomly. This robot had a certain amount of 
information after pulling the lever for the first time: this is 
a good lever to pull. In the next game this robot already knows 
which lever might be a good bet, so among the $k$ possible 
options it might choose to pull the same lever as last time. 
Next time the reward is much smaller, the robot transitioned 
states in a sense. It was in a state were the best option was 
somewhat clear, to a state in which that one lever is no longer 
the most desirable option. It went from being certain to being 
forced to explore. We might think of this process a navigating 
the branches of a tree, making a decision every time a branch 
forks of into smaller branches.

[AQUI DIAGRAMA DE ARBOL]

\section{A slightly more nuanced example}
One of the major achievements and claims to fame for 
reinforcement learning in the past decades has been its use in 
some of the most famous matches in several classic board games.  
Chess was ``mastered'' when Kasparov, then chess grandmaster, 
was beat by Deep Blue in 1997 \cite{silverchess}, but other 
board games with enormous state spaces such as Go, the game 
that originated in China, proved to be too complex for the 
techniques used by Deep Blue. Deep Blue was mainly based on 
hard-coded openings and endgames taken from other grandmaster 
games, which differs a lot from the RL approach. Go was much 
later attacked by another robot player named AlphaGo, developed 
by researchers from a company named DeepMind 
\cite{silver2017mastering}.  AlphaGo defeated Lee Sedol, the 
reigning human champion in 2016.  AlphaGo was one of the most 
notorious applications of RL and made the field even more 
popular. Since RL has proved to be very good for these kinds of 
tasks in the past, let us explore an example with a simpler 
board game.

\subsection{Duopoly, oligopoly, \ldots}
Consider a simplified version of a popular game whose name I 
avoid using as it might be copyrighted. This game consists of a 
circuit with predefined squares, some available for sale, in 
which a player is at any given moment. In order to move, a 
player spins an arrow fixed on the center of a circle that can 
stop at any of four circle segments numbered from 1 to 4. This 
is the number of squares the player moves forward. You can find 
a scaled version of this game's board in figure 
\ref{fig:miniopoly-board}. The squared numbered 1 through 8 
(colored) are available for purchase, while the white squares 
indicate a special action such as allowing the player to spin 
once more in the same turn, complete the circuit by going 
directly to the starting square and so on.

The robot might acquire that square for itself and if it lands 
on it again, gains some amount of money. The robot also 
receives money every time it completes the circuit. In this 
simplified version the robot is playing alone, but every turn 
some square not currently owned by the robot might become 
unavailable for purchase, and the robot must pay a fee if it 
lands on it from then on. If it lands on jail the robot also 
loses money, a bigger amount. As you might have guessed, the 
point of the game is to make the most amount of money. The game 
ends when the robot goes bankrupt or the amount of squares that 
the robot can buy runs out. What purchase strategy must the 
robot follow to win?
\begin{figure}[H]
	\centering
	\includegraphics[width=.75\textwidth]{img/board.pdf}
	\caption{Board for the game described earlier.}
	\label{fig:miniopoly-board}
\end{figure}

First off, unlike the slot machine-playing robot, this robot 
player cannot entirely choose which square to move to, it can 
only decide if it wants to purchase it or not once it landed on 
it.  For instance, if we look at Fig. \ref{fig:miniopoly-board} 
we see that even if the robot wanted to go to square number 8 
and buy it there is no possible way to do that. The biggest 
number it can spin is 4, and even if it got to roll again the 
furthest it could go is square 5. Let's revise the board and 
simplify by removing the squares that are not accesible on the 
first spin, and lets draw some arrows to make the transitions 
clearer.

\begin{figure}[h]
	\centering
	\includegraphics[width=.75\textwidth]{img/diagram-start.pdf}
	\caption{Squares accessible on the first turn.}
	\label{fig:miniopoly-diagram-start}
\end{figure}

In figure \ref{fig:miniopoly-diagram-start} we can see all the 
possible squares the robot might land on and then buy starting 
from the first turn. Notice how in this figure we only care 
about the starting and ending points, not the transit points, 
so that is why there are no arrows pointing to the squares that 
move the player elsewhere like Spin Again and GoTo Start.
This is the reason there is an arrow that points from ``Start'' 
to itself. This transition might happen. In fact, we know what 
it takes for this to happen: land on ``Roll Again'', and then 
land on ``GoTo Start''. In other words, this transition happens 
if the robot spins a 3 two consecutive times.

% va arriba
Notice how squares 6 and further are not included.
If the robot were to spin a 4, the square with number 3 is the 
furthest it could go. If it were to land on Roll Again and then 
spin a 4, the largest possible spin, it could move four 
squares: from Roll Again to 3, then to 4, then to GoTo Start, 
and finally to square 5, exhausting its possible moves.

If the robot were to land on Roll Again, then spin a 3, it 
would ``land'' on GoTo Start, finishing its turn back at the 
starting square. That particular move results in the robot 
going back to the start and getting paid for completing the 
circuit, without running the risk of landing on a square 
someone owns or jail and getting charged. That sounds like a 
very desirable play (for us looking at the big picture, the 
robot cannot see that far ahead yet). Since we would like this 
to happen, we might ask how likely is for that to happen.

\begin{figure}[h]
	\centering
	\includegraphics[width=\textwidth]{img/transicion.pdf}
	\caption{Transition diagram}
	\label{fig:miniopoly-transicion}
\end{figure}

Lets try to analyze how likely the robot is to end up in this 
particularly rewarding situation. For that it's often helpful 
to think of the possible steps that lead to it. On figure 
\ref{fig:miniopoly-transicion} we can see a more abstract 
diagram of the possible movements the robot might make, and 
this time we are taking into account intermediate squares. Each 
arrow represents where the robot might land after spinning, not 
necessarily where it is when the turn ends. Now, using some 
very basic probability theory we can calculate how likely it is 
for the robot to ``take that route'' so to speak. Once we do, 
we can calculate how likely a ``trajectory'' or sequence of 
arrows is.

% Quitar el p=1 en la flecha de de GS -> S
\begin{figure}[h]
	\centering
	\includegraphics[width=\textwidth]{img/transicion-markov.pdf}
	\caption{Transition diagram with transition probabilities}
	\label{fig:miniopoly-transicion-markov}
\end{figure}

At the moment it is not important why some arrows have certain 
probabilities, and some might not be terribly obvious even for 
someone acquainted with some basic probability. In diagram 
\ref{fig:miniopoly-transicion-markov} to find the probability 
of getting to a certain square (say 6 for example) we look only 
at the last arrow, as it takes into account the probability of 
the last step happening.

The robot we are trying to teach moves as the diagram above 
suggests. For each square there is a different diagram. The 
robot is not aware of this, all it knows is that it spins, and 
then is transported elsewhere and must decide whether to buy or 
not (if the square is available). All it knows is where it has 
been before and if it gained or lost money last turn. 
Crucially, landing on a square previously visited does not 
necessarily mean the reward will be the same as last time. 
Also, the reward for buying a square may come much later on or 
not at all. This student must somehow keep track of what the 
expected reward for landing somewhere might be, recording it 
somewhere and tallying up as it goes. For instance after a few 
turns it might have something fig. \ref{fig:hist-miniopoly} on 
its head to help make decisions.

% Aqui no va el square 6 porque es first turn
\begin{figure}
\centering
\includegraphics[width=\textwidth]{img/hist-miniopoly.tikz}
\label{fig:hist-miniopoly}
\caption{Boxplot of rewards obtained by visiting each square}
\end{figure}

% EXPLICAR MUCHO MEJOR
% SOLO ES UN FIRST TURN SIMULADO MUCHAS VECES CON SQUARES 
% COMPRADOS YA
Its stands to reason that as this histogram is updated the 
learning agent will be able to make progressively better 
choices. Simple as this example might be, the essence is 
similar to the powerful programs that beat world champions.

This example covers most of the core ideas of what is referred 
to as the Reinforcement Learning Problem. We will explore the 
ideas presented here in much more detail along this theses and 
crucially, focus on something we left out on this example. How 
can a purchase strategy be improved upon?

\section{Wrapping up}
So far we have developed a mental model of what to do should we 
wish to teach a robot how to drive or beat a world champion of 
Go. The rest of this thesis is dedicated to the careful 
development of the ideas here presented into the language of 
mathematics.  But beyond mere description, this exercise has 
the potential to unlock \textit{insight}. As is often said by 
legendary math communicator Grant Sanderson\footnote{From the 
YouTube channel 
\href{https://www.youtube.com/channel/UCYO_jab_esuFRV4b17AJtAw}{3blue1brown}}(loose 
quote), the point in formulating things this way is to gain a 
deeper understanding of the phenomenon. So let's dive right in.
