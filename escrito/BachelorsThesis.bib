
@article{bellman1957,
  title = {A {{Markovian Decision Process}}},
  author = {Bellman, Richard},
  date = {1957},
  journaltitle = {Journal of Mathematics and Mechanics},
  volume = {6},
  number = {5},
  eprint = {24900506},
  eprinttype = {jstor},
  pages = {679--684},
  publisher = {{Indiana University Mathematics Department}},
  issn = {0095-9057}
}

@article{Blaom2020,
  title = {{{MLJ}}: {{A Julia}} Package for Composable Machine Learning},
  author = {Blaom, Anthony D. and Kiraly, Franz and Lienart, Thibaut and Simillides, Yiannis and Arenas, Diego and Vollmer, Sebastian J.},
  date = {2020},
  journaltitle = {Journal of Open Source Software},
  volume = {5},
  number = {55},
  pages = {2704},
  publisher = {{The Open Journal}},
  doi = {10.21105/joss.02704},
  url = {https://doi.org/10.21105/joss.02704}
}

@book{breiman2017,
  title = {Classification {{And Regression Trees}}},
  author = {Breiman, Leo},
  date = {2017-10-24},
  publisher = {{Routledge}},
  location = {{New York}},
  doi = {10.1201/9781315139470},
  abstract = {The methodology used to construct tree structured rules is the focus of this monograph. Unlike many other statistical procedures, which moved from pencil and},
  isbn = {978-1-315-13947-0},
  pagetotal = {368}
}

@article{denardo1970,
  title = {On {{Linear Programming}} in a {{Markov Decision Problem}}},
  author = {Denardo, Eric V.},
  date = {1970},
  journaltitle = {Management Science},
  volume = {16},
  number = {5},
  eprint = {2628518},
  eprinttype = {jstor},
  pages = {281--288},
  publisher = {{INFORMS}},
  issn = {0025-1909},
  abstract = {This paper treats a Markov decision problem with an infinite planning horizon and no discounting. This model is analyzed by application, perhaps repeated, of a simple linear program.}
}

@article{depenoux1963,
  title = {A {{Probabilistic Production}} and {{Inventory Problem}}},
  author = {D'\'Epenoux, Fran\c{c}ois},
  date = {1963},
  journaltitle = {Management Science},
  volume = {10},
  number = {1},
  eprint = {2627210},
  eprinttype = {jstor},
  pages = {98--108},
  publisher = {{INFORMS}},
  issn = {0025-1909}
}

@book{elements2009,
  title = {The {{Elements}} of {{Statistical Learning}}},
  author = {Hastie, Trevor and Tibshirani, Robert and Friedman, Jerome},
  date = {2009},
  series = {Springer {{Series}} in {{Statistics}}},
  publisher = {{Springer}},
  location = {{New York, NY}},
  doi = {10.1007/978-0-387-84858-7},
  url = {http://link.springer.com/10.1007/978-0-387-84858-7},
  urldate = {2022-09-01},
  isbn = {978-0-387-84857-0 978-0-387-84858-7},
  keywords = {Averaging,Boosting,classification,clustering,data mining,machine learning,Projection pursuit,Random Forest,supervised learning,Support Vector Machine,unsupervised learning}
}

@thesis{farias2002thesis,
  type = {phdthesis},
  title = {The {{Linear Programming Approach}} to {{Approximate Dynamic Programming}}: {{Theory}} and {{Application}}},
  author = {family=Pucci, given=family=Farias, suffix=prefix=De, prefix=given=Daniela, useprefix=true},
  date = {2002},
  institution = {{Stanford University}}
}

@article{farias2003LP2ADP,
  title = {The {{Linear Programming Approach}} to {{Approximate Dynamic Programming}}},
  author = {family=Pucci, given=family=Farias, suffix=prefix=De, prefix=given=Daniela, useprefix=true and Van Roy, Benjamin},
  date = {2003},
  journaltitle = {Operations Research},
  volume = {51},
  number = {6},
  pages = {850--865},
  publisher = {{INFORMS}},
  issn = {0030364X, 15265463}
}

@book{intro2statslearning,
  title = {An {{Introduction}} to {{Statistical Learning}}},
  author = {James, Gareth and Witten, Daniela and Hastie, Trevor and Tibshirani, Robert},
  date = {2013},
  series = {Springer {{Texts}} in {{Statistics}}},
  volume = {103},
  publisher = {{Springer}},
  location = {{New York, NY}},
  doi = {10.1007/978-1-4614-7138-7},
  url = {http://link.springer.com/10.1007/978-1-4614-7138-7},
  urldate = {2022-09-01},
  isbn = {978-1-4614-7137-0 978-1-4614-7138-7},
  keywords = {data mining,inference,R,R software,statistical learning,supervised learning,unsupervised learning}
}

@thesis{louppe2014,
  type = {phdthesis},
  title = {Understanding Random Forests: {{From}} Theory to Practice},
  author = {Louppe, Gilles},
  date = {2014-10},
  institution = {{University of Liege, Belgium}},
  url = {https://github.com/glouppe/phd-thesis},
  pagetotal = {225}
}

@article{moisescu-parejaa,
  title = {Lecture 2: {{Bellman}} Operator, {{Banach}}'s Fixed Point, Solving {{MDPs}} - {{SUMS}} 707 - {{Basic Reinforcement Learning}}},
  author = {Moisescu-Pareja, Gabriela and Nguyen, Viet},
  pages = {53},
  langid = {english}
}

@thesis{nadeemward2021,
  type = {mathesis},
  title = {Linear {{Programming}} in {{Reinforcement Learning}}},
  author = {Nadeem Ward, Patrick},
  date = {2021-04},
  institution = {{McGill University}},
  location = {{Montreal, Canada}},
  langid = {english},
  pagetotal = {73}
}

@book{puterman2014,
  title = {Markov Decision Processes: Discrete Stochastic Dynamic Programming},
  shorttitle = {Markov Decision Processes},
  author = {Puterman, Martin L.},
  date = {2014-08-28},
  publisher = {{John Wiley \& Sons}},
  abstract = {The Wiley-Interscience Paperback Series consists of selected books that have been made more accessible to consumers in an effort to increase global appeal and general circulation. With these new unabridged softcover volumes, Wiley hopes to extend the lives of these works by making them available to future generations of statisticians, mathematicians, and scientists. "This text is unique in bringing together so many results hitherto found only in part in other texts and papers. . . . The text is fairly self-contained, inclusive of some basic mathematical results needed, and provides a rich diet of examples, applications, and exercises. The bibliographical material at the end of each chapter is excellent, not only from a historical perspective, but because it is valuable for researchers in acquiring a good perspective of the MDP research potential."\textemdash Zentralblatt fur Mathematik ". . . it is of great value to advanced-level students, researchers, and professional practitioners of this field to have now a complete volume (with more than 600 pages) devoted to this topic. . . . Markov Decision Processes: Discrete Stochastic Dynamic Programming represents an up-to-date, unified, and rigorous treatment of theoretical and computational aspects of discrete-time Markov decision processes."\textemdash Journal of the American Statistical Association},
  isbn = {978-1-118-62587-3},
  langid = {ngerman},
  pagetotal = {615},
  keywords = {Mathematics / General,Mathematics / Probability & Statistics / General,Mathematics / Probability & Statistics / Stochastic Processes}
}

@misc{raoRL4F,
  title = {Foundations of {{Reinforcement Learning}} with {{Applications}} in {{Finance}}},
  author = {Rao, Ashwin and Jelvis, Tikhon},
  date = {2022},
  publisher = {{Stanford University}},
  langid = {english}
}

@article{scikit-learn,
  title = {Scikit-Learn: {{Machine}} Learning in {{Python}}},
  author = {Pedregosa, Fabian and Varoquaux, Gael and Gramfort, Alexandre and Michel, Vincent and Thirion, Bertrand and Grisel, Olivier and Blondel, Mathieu and Prettenhofer, Peter and Weiss, Ron and Dubourg, Vincent and Vanderplas, Jake and Passos, Alexandre and Cournapeau, David and Brucher, Matthieu and Perrot, M. and Duchesnay, Edouard},
  date = {2011},
  journaltitle = {Journal of Machine Learning Research},
  volume = {12},
  pages = {2825--2830}
}

@misc{silver2015,
  title = {Lectures on {{Reinforcement Learning}}},
  author = {Silver, David},
  date = {2015},
  annotation = {Published: \textsc{url:} https://www.davidsilver.uk/teaching/}
}

@misc{silver2017mastering,
  title = {Mastering {{Chess}} and {{Shogi}} by {{Self-Play}} with a {{General Reinforcement Learning Algorithm}}},
  author = {Silver, David and Hubert, Thomas and Schrittwieser, Julian and Antonoglou, Ioannis and Lai, Matthew and Guez, Arthur and Lanctot, Marc and Sifre, Laurent and Kumaran, Dharshan and Graepel, Thore and Lillicrap, Timothy and Simonyan, Karen and Hassabis, Demis},
  date = {2017},
  annotation = {\_eprint: 1712.01815}
}

@article{silverchess,
  title = {A General Reinforcement Learning Algorithm That Masters Chess, Shogi, and {{Go}} through Self-Play},
  author = {Silver, David and Hubert, Thomas and Schrittwieser, Julian and Antonoglou, Ioannis and Lai, Matthew and Guez, Arthur and Lanctot, Marc and Sifre, Laurent and Kumaran, Dharshan and Graepel, Thore and Lillicrap, Timothy and Simonyan, Karen and Hassabis, Demis},
  date = {2018},
  journaltitle = {Science},
  volume = {362},
  number = {6419},
  pages = {1140--1144},
  doi = {10.1126/science.aar6404},
  url = {https://www.science.org/doi/abs/10.1126/science.aar6404}
}

@inproceedings{sklearn_api,
  title = {{{API}} Design for Machine Learning Software: Experiences from the Scikit-Learn Project},
  booktitle = {{{ECML PKDD}} Workshop: {{Languages}} for Data Mining and Machine Learning},
  author = {Buitinck, Lars and Louppe, Gilles and Blondel, Mathieu and Pedregosa, Fabian and Mueller, Andreas and Grisel, Olivier and Niculae, Vlad and Prettenhofer, Peter and Gramfort, Alexandre and Grobler, Jaques and Layton, Robert and VanderPlas, Jake and Joly, Arnaud and Holt, Brian and Varoquaux, Ga\"el},
  date = {2013},
  pages = {108--122}
}

@article{survey-MDP-Apps,
  title = {A {{Survey}} of {{Applications}} of {{Markov Decision Processes}}},
  author = {White, D. J.},
  date = {1993},
  journaltitle = {The Journal of the Operational Research Society},
  volume = {44},
  number = {11},
  pages = {1073--1096},
  publisher = {{Palgrave Macmillan Journals}},
  issn = {01605682, 14769360}
}

@book{SuttonBarto,
  title = {Reinforcement Learning: {{An}} Introduction},
  author = {Sutton, Richard S. and Barto, Andrew G.},
  date = {2020},
  series = {Adaptive {{Computation}} and {{Machine Learning}}},
  edition = {2},
  publisher = {{MIT press}},
  isbn = {978-0-262-03924-6}
}

@inproceedings{XGBoost,
  title = {{{XGBoost}}: {{A Scalable Tree Boosting System}}},
  shorttitle = {{{XGBoost}}},
  booktitle = {Proceedings of the 22nd {{ACM SIGKDD International Conference}} on {{Knowledge Discovery}} and {{Data Mining}}},
  author = {Chen, Tianqi and Guestrin, Carlos},
  date = {2016-08-13},
  eprint = {1603.02754},
  eprinttype = {arxiv},
  primaryclass = {cs},
  pages = {785--794},
  doi = {10.1145/2939672.2939785},
  url = {http://arxiv.org/abs/1603.02754},
  urldate = {2022-09-05},
  abstract = {Tree boosting is a highly effective and widely used machine learning method. In this paper, we describe a scalable end-to-end tree boosting system called XGBoost, which is used widely by data scientists to achieve state-of-the-art results on many machine learning challenges. We propose a novel sparsity-aware algorithm for sparse data and weighted quantile sketch for approximate tree learning. More importantly, we provide insights on cache access patterns, data compression and sharding to build a scalable tree boosting system. By combining these insights, XGBoost scales beyond billions of examples using far fewer resources than existing systems.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning}
}

@article{xiong,
  title = {Learning {{Decision Trees}} with {{Reinforcement Learning}}},
  author = {Xiong, Zheng and Zhang, Wenpeng and Zhu, Wenwu},
  pages = {5},
  abstract = {Decision trees are usually learned by heuristic methods like greedy search, which only considers immediate information gain at the current splitting node and often results in sub-optimal solutions in a constrained search space. In this paper, to overcome this problem, we propose a reinforcement learning approach to automatically search for splitting strategies in the global search space based on the evaluation of long-term payoff. Empirically, decision trees generated by our method outperform those generated by commonly used greedy search methods under the same hyper-parameter setting.},
  langid = {english}
}


